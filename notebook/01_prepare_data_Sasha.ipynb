{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "medieval-danger",
   "metadata": {},
   "source": [
    "## Etude des sujets des amendements PLFSS - Sasha\n",
    "\n",
    "## Préparation des données"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "encouraging-simpson",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!python -m pip install --upgrade pip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "direct-receipt",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install -q --upgrade pandas  spacy  more_itertools"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "brief-telescope",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Put these at the top of every notebook, to get automatic reloading and inline plotting\n",
    "%reload_ext autoreload\n",
    "%autoreload 2\n",
    "from IPython.core.interactiveshell import InteractiveShell\n",
    "InteractiveShell.ast_node_interactivity = \"all\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "occasional-logan",
   "metadata": {},
   "outputs": [],
   "source": [
    "!python -m spacy download fr_core_news_sm > /dev/null"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "structural-windsor",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing modules\n",
    "import pandas as pd\n",
    "#amdt = pd.read_csv('https://github.com/leximpact/donnees-extraites-assemblee/blob/main/textes_amendements_nouveaux_articles_plfss_2020-2021.csv')\n",
    "amdt = pd.read_csv('https://raw.githubusercontent.com/leximpact/donnees-extraites-assemblee/main/textes_amendements_nouveaux_articles_plfss_2020-2021.csv')\n",
    "amdt.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "continued-doctrine",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(amdt) #Nombre total d'amendements"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "incredible-search",
   "metadata": {},
   "outputs": [],
   "source": [
    "#On regroupe dans un même texte chaque dispositif et son exposé sommaire\n",
    "amdt['texte'] = amdt['dispositif'] + amdt['exposeSommaire'] "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "improving-boutique",
   "metadata": {},
   "source": [
    "## Nettoyage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "controlling-treasure",
   "metadata": {},
   "outputs": [],
   "source": [
    "#pip install nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "returning-progressive",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "nltk.download('stopwords')\n",
    "nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "unusual-cathedral",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "developmental-japan",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Tokenisation := découpage du texte en listes de mots\n",
    "from nltk.tokenize import word_tokenize\n",
    "tokenized = [ word_tokenize(text) for text in amdt['texte'] ] #return_token"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "surprising-effort",
   "metadata": {},
   "source": [
    "# Normalization"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "gorgeous-marshall",
   "metadata": {},
   "source": [
    "### Retrait des mots de liaison (_stopword_) et de la ponctuation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "average-vector",
   "metadata": {},
   "outputs": [],
   "source": [
    "# #Removing punctuation AND Casing\n",
    "#( Casing: Est-ce vraiment utile ? Est-ce qu'on ne va pas perdre les Acronymes de vue ?)\n",
    "new_tokenized = []\n",
    "for token in tokenized:\n",
    "    new_tokenized.append([ word.lower() for word in token if word.isalpha()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "hairy-train",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(new_tokenized[4])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "altered-chorus",
   "metadata": {},
   "outputs": [],
   "source": [
    "#STOP WORDS\n",
    "import spacy\n",
    "#On importe les mots \"inutiles\" (stopwords) du langage français depuis Space -ET- NLTK\n",
    "from nltk.corpus import stopwords \n",
    "stop_words = stopwords.words(\"french\")\n",
    "\n",
    "from spacy.lang.fr.stop_words import STOP_WORDS as fr_stop\n",
    "for word in fr_stop:\n",
    "        stop_words.append(word)\n",
    "del(word)\n",
    "stop_words = list(stop_words)\n",
    "\n",
    "#On importe nos propres stopwords depuis le fichier CSV\n",
    "SW = pd.read_csv('Added_stop_words.csv')\n",
    "SW = list(SW.StopWords)\n",
    "final_SW = stop_words+ SW\n",
    "\n",
    "#On enlève les doublons\n",
    "df = pd.DataFrame({\"StopWords\" : final_SW})\n",
    "final_SW = df.drop_duplicates()\n",
    "final_SW = list(final_SW.StopWords)\n",
    "print(\"NOS MOTS 'INUTILES' : \\n \\n\", final_SW ,'\\n')\n",
    "\n",
    "#STOP WORD REMOVAL\n",
    "print(\"AVANT : \\n \\n\", tokenized[4], \"\\n\")\n",
    "tokenized = []\n",
    "for token in new_tokenized:\n",
    "    tokenized.append([ word for word in token if word not in final_SW])\n",
    "print(\"APRÈS : \\n \\n\", tokenized[4], \"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "neural-lawsuit",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "nltk.download('wordnet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "accompanied-dating",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Lemmatization: on reduit les mots à leur racine\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "#amdt_clean= []\n",
    "#for token in tokenized:\n",
    "#    amdt_clean.append([lemmatizer.lemmatize(word) for word in token])\n",
    "#print(amdt_clean[4])\n",
    "\n",
    "#La lemmatization de nltk est nulle en français. Il faudrait pouvoir réussir à importer celle de Spacy\n",
    "import spacy\n",
    "nlp = spacy.load(\"fr_core_news_md\")\n",
    "\n",
    "#amdt_clean= []\n",
    "#for token in tokenized:\n",
    "#    amdt_clean.append([nlp(word) for word in token])\n",
    "#print(\"Ci-dessous les mots réduits à leur racine: \\n \\n\", amdt_clean[4])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "automotive-representation",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Cellule de test : Spacy ne marche pas\n",
    "import spacy\n",
    "nlp = spacy.load(\"fr_core_news_md\")\n",
    "print(nlp(u'mangées'))\n",
    "print(nlp('mangé'))\n",
    "print(nlp('mangés'))\n",
    "print(nlp('mangez'))\n",
    "print(nlp('mangeons'))\n",
    "print(nlp('joliment'))\n",
    "print(nlp('eaten'))\n",
    "print(nlp('voiles'))\n",
    "print(nlp('was'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ecological-animal",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Cellule de test : nltk marche mais pas ouf car mots coupés\n",
    "from nltk.stem.snowball import FrenchStemmer\n",
    "stemmer = FrenchStemmer()\n",
    "stemmer.stem('voudrais')\n",
    "\n",
    "#Cellule de test\n",
    "print(stemmer.stem('mangée'))\n",
    "print(stemmer.stem('mangées'))\n",
    "print(stemmer.stem('mangé'))\n",
    "print(stemmer.stem('mangés'))\n",
    "print(stemmer.stem('mangez'))\n",
    "print(stemmer.stem('mangeons'))\n",
    "print(stemmer.stem('joliment'))\n",
    "print(stemmer.stem('eaten'))\n",
    "print(stemmer.stem('voiles'))\n",
    "print(stemmer.stem('was'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "robust-genetics",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: FrenchLefffLemmatizer in /Users/sasha/.local/share/virtualenvs/openfisca/lib/python3.7/site-packages (0.3)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install FrenchLefffLemmatizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "exempt-christianity",
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: '/Users/sasha/.local/share/virtualenvs/openfisca/lib/python3.7/site-packages/french_lefff_lemmatizer/data/lefff-3.4.mlex'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-3-afb81c31d9e5>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m#Cellule de test : FrenchLeffLemmatizer Claude Coulomb : pour l'instant ne tourne pas\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mfrench_lefff_lemmatizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfrench_lefff_lemmatizer\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mFrenchLefffLemmatizer\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mlemmatizer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mFrenchLefffLemmatizer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlemmatizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlemmatize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'mangée'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/share/virtualenvs/openfisca/lib/python3.7/site-packages/french_lefff_lemmatizer/french_lefff_lemmatizer.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, lefff_file_path, lefff_additional_file_path, with_additional_file, load_only_pos)\u001b[0m\n\u001b[1;32m     59\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTRACE\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     60\u001b[0m         \u001b[0mset_pos_triplets\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 61\u001b[0;31m         \u001b[0;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mLEFFF_FILE_STORAGE\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mencoding\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'utf-8'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mlefff_file\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     62\u001b[0m             \u001b[0;32mfor\u001b[0m \u001b[0ma_line\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mlefff_file\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     63\u001b[0m                 \u001b[0mline_parts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0ma_line\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'\\t'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '/Users/sasha/.local/share/virtualenvs/openfisca/lib/python3.7/site-packages/french_lefff_lemmatizer/data/lefff-3.4.mlex'"
     ]
    }
   ],
   "source": [
    "#Cellule de test : FrenchLeffLemmatizer Claude Coulomb : pour l'instant ne tourne pas\n",
    "from french_lefff_lemmatizer.french_lefff_lemmatizer import FrenchLefffLemmatizer\n",
    "lemmatizer = FrenchLefffLemmatizer()\n",
    "\n",
    "print(lemmatizer.lemmatize('mangée'))\n",
    "print(lemmatizer.lemmatize('mangées'))\n",
    "print(lemmatizer.lemmatize('mangé'))\n",
    "print(lemmatizer.lemmatize('mangés'))\n",
    "print(lemmatizer.lemmatize('mangez'))\n",
    "print(lemmatizer.lemmatize('mangeons'))\n",
    "print(lemmatizer.lemmatize('joliment'))\n",
    "print(lemmatizer.lemmatize('eaten'))\n",
    "print(lemmatizer.lemmatize('voiles'))\n",
    "print(lemmatizer.lemmatize('was'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "average-horror",
   "metadata": {},
   "outputs": [],
   "source": [
    "#pip install spacy-lefff"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "applied-mixer",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2021-03-02 15:40:10,906 - spacy_lefff.lefff - INFO - New LefffLemmatizer instantiated.\n",
      "2021-03-02 15:40:10,907 - spacy_lefff.lefff - INFO - Token lefff_lemma already registered\n",
      "2021-03-02 15:40:10,908 - spacy_lefff.lefff - INFO - Reading lefff data...\n",
      "2021-03-02 15:40:11,921 - spacy_lefff.lefff - INFO - Successfully loaded lefff lemmatizer\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "lemmatize() missing 1 required positional argument: 'pos'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-11-bc085438076e>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0mnlp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd_pipe\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'tagger'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'pos'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mafter\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'parser'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 13\u001b[0;31m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlemmatizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlemmatize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'mangée'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     14\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlemmatizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlemmatize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'mangées'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlemmatizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlemmatize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'mangé'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: lemmatize() missing 1 required positional argument: 'pos'"
     ]
    }
   ],
   "source": [
    "#Cellule de test : LeffLemmatizer (V2 from Claude Coulomb) : pour l'instant ne tourne pas car il faut implementer\n",
    "# le pos tag\n",
    "import spacy\n",
    "from spacy_lefff import LefffLemmatizer\n",
    "from spacy_lefff import POSTagger\n",
    "#nlp = spacy.load(\"fr_core_news_sm\")\n",
    "nlp = spacy.load(\"fr_core_news_md\")\n",
    "lemmatizer = LefffLemmatizer()#Initialisation\n",
    "#nlp.add_pipe(fr.lemmatizer)\n",
    "nlp.add_pipe('lemmatizer', name='lefff')\n",
    "nlp.add_pipe('tagger', name='pos', after='parser')\n",
    "\n",
    "print(lemmatizer.lemmatize('mangée', ))\n",
    "print(lemmatizer.lemmatize('mangées'))\n",
    "print(lemmatizer.lemmatize('mangé'))\n",
    "print(lemmatizer.lemmatize('mangés'))\n",
    "print(lemmatizer.lemmatize('mangez'))\n",
    "print(lemmatizer.lemmatize('mangeons'))\n",
    "print(lemmatizer.lemmatize('joliment'))\n",
    "print(lemmatizer.lemmatize('eaten'))\n",
    "print(lemmatizer.lemmatize('voiles'))\n",
    "print(lemmatizer.lemmatize('was'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "prostate-lesson",
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: '/Users/sasha/.local/share/virtualenvs/openfisca/lib/python3.7/site-packages/french_lefff_lemmatizer/data/lefff-3.4.mlex'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-12-b8fde5ce9df1>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mspacy\u001b[0m  \u001b[0;31m#pip install FrenchLefffLemmatizer\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mspacy_lefff\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mLefffLemmatizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mPOSTagger\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0mlemmatizer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mFrenchLefffLemmatizer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m#Initialisation\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mspacy\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0mnlp\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mspacy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"fr_dep_news_trf\"\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m# Le plus précis\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/share/virtualenvs/openfisca/lib/python3.7/site-packages/french_lefff_lemmatizer/french_lefff_lemmatizer.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, lefff_file_path, lefff_additional_file_path, with_additional_file, load_only_pos)\u001b[0m\n\u001b[1;32m     59\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTRACE\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     60\u001b[0m         \u001b[0mset_pos_triplets\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 61\u001b[0;31m         \u001b[0;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mLEFFF_FILE_STORAGE\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mencoding\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'utf-8'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mlefff_file\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     62\u001b[0m             \u001b[0;32mfor\u001b[0m \u001b[0ma_line\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mlefff_file\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     63\u001b[0m                 \u001b[0mline_parts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0ma_line\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'\\t'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '/Users/sasha/.local/share/virtualenvs/openfisca/lib/python3.7/site-packages/french_lefff_lemmatizer/data/lefff-3.4.mlex'"
     ]
    }
   ],
   "source": [
    "#Cellule de test :  ICI ON DECIDE DE CREER LE PIPELINE SPACY\n",
    "import spacy  #pip install FrenchLefffLemmatizer\n",
    "from spacy_lefff import LefffLemmatizer, POSTagger\n",
    "\n",
    "nlp = spacy.load(\"fr_dep_news_trf\") # Le plus précis\n",
    "nlp = spacy.load(\"fr_core_news_md\") #Le plus efficace\n",
    "pos = POSTagger()\n",
    "#french_lemmatizer = LefffLemmatizer(after_melt=True, default=True)\n",
    "\n",
    "nlp.add_pipe('tagger', name='pos', after='parser')\n",
    "#nlp.add_pipe(pos, name='pos', after='parser')\n",
    "#nlp.add_pipe(french_lemmatizer, name='lefff', after='pos')\n",
    "\n",
    "nlp.add_pipe('fr.lemmatizer',name='lefff', after='pos')\n",
    "\n",
    "doc = nlp(u\"Apple cherche a acheter une startup anglaise pour 1 milliard de dollard\")\n",
    "for d in doc:\n",
    "    print(d.text, d.pos_, d._.melt_tagger, d._.lefff_lemma, d.tag_, d.lemma_)\n",
    "\n",
    "\n",
    "print(nlp(u'mangées'))\n",
    "print(nlp('mangé'))\n",
    "print(nlp('mangés'))\n",
    "print(nlp('mangez'))\n",
    "print(nlp('mangeons'))\n",
    "print(nlp('joliment'))\n",
    "print(nlp('eaten'))\n",
    "print(nlp('voiles'))\n",
    "print(nlp('was'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "threatened-sacramento",
   "metadata": {},
   "outputs": [],
   "source": [
    "pip list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "neither-county",
   "metadata": {},
   "outputs": [],
   "source": [
    "#On enregistre tous les textes en version \"clean\" pour pouvoir réutiliser cette data plus tard\n",
    "del(df)\n",
    "df = pd.DataFrame(data = amdt_clean )\n",
    "df = df.fillna('') \n",
    "print(df.head(10))\n",
    "df.to_csv('amdements_cleaned.csv')  #Chaque ligne est un amendement"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cleared-fishing",
   "metadata": {},
   "source": [
    "# Les mots les plus cités"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bronze-mason",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Counting\n",
    "from collections import Counter\n",
    "def most_common_words(amdt_clean, N):\n",
    "    freq = []\n",
    "    words = []\n",
    "    for token in amdt_clean:\n",
    "        bow = Counter(token)\n",
    "        freq.append(bow.most_common(N)) \n",
    "        #print(bow.most_common(N))\n",
    "        \n",
    "        words.append([bow.most_common(N)[i][0] for i in range(N-1)])\n",
    "    return pd.DataFrame(freq), words\n",
    "\n",
    "#On ne garde que les N mots les plus utilisés pour chaque amendement\n",
    "N = 10\n",
    "freq, words = most_common_words(amdt_clean, N)\n",
    "#On sauvegarde cette donnée dans un fichier .csv \n",
    "#dont les lignes sont les amdts et les colonnes sont les mots et leur fréquence\n",
    "freq.to_csv('most_common_{nb}_words_per_amdt.csv'.format(nb = N))\n",
    "\n",
    "print(\"Ci-dessous un exemple des mots les plus utilisés sur 5 amendements: \\n \\n \", freq.head(5))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "sound-alias",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analyse des mots du corpus dans sa globalité\n",
    "words = pd.DataFrame(words)\n",
    "mots_uniques = []\n",
    "for c in words:\n",
    "    temp = list (words[c].unique())\n",
    "    for word in temp:\n",
    "        mots_uniques.append( word )  #List of unique words\n",
    "        \n",
    "#On enlève les doublons\n",
    "mots_uniques = list(dict.fromkeys(mots_uniques))\n",
    "mots_uniques.sort()\n",
    "print( len(mots_uniques) )\n",
    "print(mots_uniques)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "communist-blanket",
   "metadata": {},
   "source": [
    "# TF IDF\n",
    "Term frequency - inverse document frequency"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "listed-disorder",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Mise sous forme 'corpus' (une liste de tous les textes)\n",
    "corpus = []\n",
    "for amdt1 in amdt_clean:\n",
    "    temp = ' '.join(amdt1)\n",
    "    corpus.append(temp)\n",
    "    temp = ''\n",
    "    \n",
    "#Vectorization - Term Frequency in Global Corpus\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "vectorizer = CountVectorizer()\n",
    "tf = vectorizer.fit_transform(corpus)\n",
    "\n",
    "#print(corpus[2:4])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "intended-authentication",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Features\n",
    "feature_names = vectorizer.get_feature_names()\n",
    "print(feature_names[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ordinary-loading",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Génération du corpus index\n",
    "import re\n",
    "corpus_index = amdt['uid'].tolist()\n",
    "print(corpus_index[:5])\n",
    "#amdt['uid'].nunique() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "prostate-economy",
   "metadata": {},
   "outputs": [],
   "source": [
    "## LAST STOP HERE: POURQUOI A-T-ON UNIQUEMENT DES NAN ?\n",
    "\n",
    "#IDF - Inverse Document Frequencies\n",
    "import numpy as np\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "# initialize and fit TfidfVectorizer\n",
    "vectorizer = TfidfVectorizer(norm=None)\n",
    "tf_idf_scores = vectorizer.fit_transform(corpus)\n",
    "#print(tf_idf_scores)\n",
    "# get vocabulary of terms\n",
    "feature_names = vectorizer.get_feature_names()\n",
    "#print(type(tf_idf_scores.todense()))\n",
    "print (pd.DataFrame(tf_idf_scores.todense()) )  #ICI on a bien les bonnes valeurs\n",
    "print(pd.DataFrame(tf_idf_scores.todense()[0][0]))\n",
    "#print(feature_names)\n",
    "# create pandas DataFrame with tf-idf scores\n",
    "df_tf_idf = pd.DataFrame(data = pd.DataFrame(tf_idf_scores.todense()), index=corpus_index, columns=feature_names)\n",
    "print(df_tf_idf)\n",
    "#df_tf_idf.fillna(0)\n",
    "df_tf_idf.replace(np.nan, 0)\n",
    "#df_tf_idf.reset_index() \n",
    "#df_tf_idf.rename_axis('ID') \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "lesbian-orchestra",
   "metadata": {},
   "outputs": [],
   "source": [
    "for n in range(5):#range(len(df_tf_idf)):\n",
    "    #print(n)\n",
    "    #print(id)\n",
    "    main = pd.DataFrame()\n",
    "    main['Most_frequent_words'] = feature_names\n",
    "    main['Word_occurrence'] = df_tf_idf.iloc[n]  # MAIN ISSUE HERE: Series definition not taking float but convert2 nan\n",
    "    print(max(df_tf_idf.iloc[n]))\n",
    "    print(len(df_tf_idf.iloc[n]))\n",
    "    print(type(df_tf_idf.iloc[n]))\n",
    "    print(max(main['Word_occurrence']))\n",
    "    print(type(main['Word_occurrence']))\n",
    "    print(type(main))\n",
    "    print(main.head(5))\n",
    "    main.sort_values(by =['Word_occurrence'], inplace = True, ascending = False, na_position='last')\n",
    "    print(main.head(5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "hispanic-running",
   "metadata": {},
   "outputs": [],
   "source": [
    "#exclude\n",
    "#Organizing most common words !Il faut transposer\n",
    "# tuple_mots = ()\n",
    "# for n in range(len(df_tf_idf)):\n",
    "#     print(n)\n",
    "#     id = df_tf_idf.index[n]\n",
    "#     print(id)\n",
    "#     #Creating a DF per amendment\n",
    "#     main = pd.DataFrame()\n",
    "#     main['Mots principaux'] = df_tf_idf.columns.T\n",
    "#     main['Occurence'] = df_tf_idf.iloc[n]\n",
    "    \n",
    "    \n",
    "#     #main['mots'] = df_tf_idf['ID']\n",
    "    \n",
    "#     print(max(df_tf_idf.index[n]))\n",
    "#     #print(main.head(5))\n",
    "#     #main.sort_values(main[id])\n",
    "\n",
    "#     #main = main.iloc[:25] #On ne garde que les 25 mots les plus utilisés par amendement\n",
    "#     #tuple_mots.append(main)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "infinite-kitchen",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_tf_idf.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fundamental-terrace",
   "metadata": {},
   "outputs": [],
   "source": [
    "id = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "light-rubber",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_tf_idf.index[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dedicated-georgia",
   "metadata": {},
   "outputs": [],
   "source": [
    "test = df_tf_idf.index[id]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "educational-valley",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "openfisca",
   "language": "python",
   "name": "openfisca"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
