{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "standing-strategy",
   "metadata": {},
   "source": [
    "## Etude des sujets des amendements PLFSS\n",
    "\n",
    "## Préparation des données"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "mathematical-discipline",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!python -m pip install --upgrade pip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "nominated-amount",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install -q --upgrade pandas  spacy  more_itertools"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "numerical-saint",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Put these at the top of every notebook, to get automatic reloading and inline plotting\n",
    "%reload_ext autoreload\n",
    "%autoreload 2\n",
    "from IPython.core.interactiveshell import InteractiveShell\n",
    "InteractiveShell.ast_node_interactivity = \"all\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "classical-format",
   "metadata": {},
   "outputs": [],
   "source": [
    "!python -m spacy download fr_core_news_sm > /dev/null"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "sensitive-minneapolis",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>texteLegislatifUid</th>\n",
       "      <th>uid</th>\n",
       "      <th>avantAApres</th>\n",
       "      <th>dispositif</th>\n",
       "      <th>exposeSommaire</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>PRJLANR5L15B2296</td>\n",
       "      <td>AMANR5L15PO420120B2296P0D1N000001</td>\n",
       "      <td>A</td>\n",
       "      <td>I. – Après l’alinéa 13, insérer l’alinéa suiv...</td>\n",
       "      <td>La mise en place d’un accord d’intéressement d...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>PRJLANR5L15B2296</td>\n",
       "      <td>AMANR5L15PO420120B2296P0D1N000002</td>\n",
       "      <td>Apres</td>\n",
       "      <td>À la première phrase du premier alinéa de l’ar...</td>\n",
       "      <td>L’article L 531‑2 du Code de la Sécurité Socia...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>PRJLANR5L15B2296</td>\n",
       "      <td>AMANR5L15PO420120B2296P0D1N000005</td>\n",
       "      <td>A</td>\n",
       "      <td>Compléter l’alinéa 17 par la phrase suivante :...</td>\n",
       "      <td>S’il est louable d’expérimenter des dispositif...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>PRJLANR5L15B2296</td>\n",
       "      <td>AMANR5L15PO420120B2296P0D1N000008</td>\n",
       "      <td>A</td>\n",
       "      <td>Après l’alinéa 8, insérer les cinq alinéas sui...</td>\n",
       "      <td>Cet amendement permet d’amplifier la portée de...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>PRJLANR5L15B2296</td>\n",
       "      <td>AMANR5L15PO420120B2296P0D1N000010</td>\n",
       "      <td>Apres</td>\n",
       "      <td>Le premier alinéa de l’article L. 521‑1 du cod...</td>\n",
       "      <td>Pendant plus de cinquante ans, notre politique...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  texteLegislatifUid                                uid avantAApres  \\\n",
       "0   PRJLANR5L15B2296  AMANR5L15PO420120B2296P0D1N000001           A   \n",
       "1   PRJLANR5L15B2296  AMANR5L15PO420120B2296P0D1N000002       Apres   \n",
       "2   PRJLANR5L15B2296  AMANR5L15PO420120B2296P0D1N000005           A   \n",
       "3   PRJLANR5L15B2296  AMANR5L15PO420120B2296P0D1N000008           A   \n",
       "4   PRJLANR5L15B2296  AMANR5L15PO420120B2296P0D1N000010       Apres   \n",
       "\n",
       "                                          dispositif  \\\n",
       "0   I. – Après l’alinéa 13, insérer l’alinéa suiv...   \n",
       "1  À la première phrase du premier alinéa de l’ar...   \n",
       "2  Compléter l’alinéa 17 par la phrase suivante :...   \n",
       "3  Après l’alinéa 8, insérer les cinq alinéas sui...   \n",
       "4  Le premier alinéa de l’article L. 521‑1 du cod...   \n",
       "\n",
       "                                      exposeSommaire  \n",
       "0  La mise en place d’un accord d’intéressement d...  \n",
       "1  L’article L 531‑2 du Code de la Sécurité Socia...  \n",
       "2  S’il est louable d’expérimenter des dispositif...  \n",
       "3  Cet amendement permet d’amplifier la portée de...  \n",
       "4  Pendant plus de cinquante ans, notre politique...  "
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Importing modules\n",
    "import pandas as pd\n",
    "#amdt = pd.read_csv('textes_amendements_nouveaux_articles_plfss_2020-2021.csv')\n",
    "amdt = pd.read_csv('https://github.com/leximpact/donnees-extraites-assemblee/raw/main/textes_amendements_nouveaux_articles_plfss_2020-2021.csv')\n",
    "amdt.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "proved-backing",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4797"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(amdt) #Nombre total d'amendements"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "disabled-possibility",
   "metadata": {},
   "outputs": [],
   "source": [
    "#On regroupe dans un même texte chaque dispositif et son exposé sommaire\n",
    "amdt['texte'] = amdt['dispositif'] + amdt['exposeSommaire'] "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "intensive-commercial",
   "metadata": {},
   "source": [
    "## Nettoyage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "american-conversation",
   "metadata": {},
   "outputs": [],
   "source": [
    "#pip install nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "suburban-michael",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to /home/ben/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /home/ben/nltk_data...\n",
      "[nltk_data]   Unzipping tokenizers/punkt.zip.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('stopwords')\n",
    "nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "insured-attempt",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Tokenisation := découpage du texte en listes de mots\n",
    "from nltk.tokenize import word_tokenize\n",
    "tokenized = [ word_tokenize(text) for text in amdt['texte'] ] #return_token"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "applicable-population",
   "metadata": {},
   "source": [
    "# Normalization"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "limiting-thursday",
   "metadata": {},
   "source": [
    "### Retrait des mots de liaison (_stopword_) et de la ponctuation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "freelance-subdivision",
   "metadata": {},
   "outputs": [],
   "source": [
    "# #Removing punctuation AND Casing\n",
    "#( Casing: Est-ce vraiment utile ? Est-ce qu'on ne va pas perdre les Acronymes de vue ?)\n",
    "new_tokenized = []\n",
    "for token in tokenized:\n",
    "    new_tokenized.append([ word.lower() for word in token if word.isalpha()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "sacred-skating",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['le', 'premier', 'alinéa', 'de', 'l', 'article', 'du', 'code', 'de', 'la', 'sécurité', 'sociale', 'est', 'complété', 'par', 'une', 'phrase', 'ainsi', 'rédigée', 'elles', 'sont', 'universelles', 'pendant', 'plus', 'de', 'cinquante', 'ans', 'notre', 'politique', 'familiale', 'a', 'reposé', 'sur', 'le', 'principe', 'de', 'l', 'universalité', 'cela', 'signifie', 'qu', 'elle', 's', 'adressait', 'à', 'tous', 'les', 'français', 'sans', 'distinction', 'sociale', 'elle', 'reposait', 'sur', 'l', 'idée', 'que', 'chaque', 'enfant', 'à', 'naître', 'est', 'une', 'chance', 'et', 'une', 'richesse', 'pour', 'la', 'france', 'pour', 'son', 'avenir', 'quel', 'que', 'soient', 'les', 'ressources', 'dont', 'disposent', 'les', 'mettre', 'en', 'place', 'ce', 'principe', 'd', 'universalité', 'la', 'politique', 'familiale', 'appelle', 'des', 'outils', 'dits', 'de', 'redistribution', 'horizontale', 'c', 'des', 'mécanismes', 'de', 'solidarité', 'des', 'familles', 'sans', 'enfant', 'envers', 'les', 'familles', 'avec', 'enfants', 'pour', 'que', 'quel', 'que', 'soit', 'les', 'revenus', 'des', 'parents', 'la', 'naissance', 'd', 'un', 'enfant', 'n', 'ait', 'pas', 'pour', 'effet', 'de', 'porter', 'atteinte', 'à', 'leur', 'niveau', 'de', 'ce', 'principe', 'd', 'universalité', 'a', 'été', 'mis', 'à', 'mal', 'sous', 'le', 'précédent', 'quinquennat', 'notamment', 'à', 'travers', 'la', 'modulation', 'des', 'allocations', 'familiales', 'et', 'les', 'baisses', 'successives', 'du', 'quotient', 'familial', 'faisant', 'ainsi', 'de', 'la', 'politique', 'familiale', 'une', 'politique', 'sociale', 'comme', 'les', 'loi', 'de', 'financement', 'de', 'la', 'sécurité', 'sociale', 'pour', 'a', 'choisi', 'de', 'ne', 'pas', 'revenir', 'sur', 'cette', 'modulation', 'des', 'allocations', 'familiales', 'qui', 'a', 'pour', 'conséquence', 'principale', 'la', 'baisse', 'de', 'la', 'natalité', 'dans', 'notre', 'le', 'présent', 'amendement', 'vise', 'à', 'réintroduire', 'ce', 'principe', 'd', 'universalité', 'base', 'même', 'de', 'la', 'politique', 'familiale', 'française', 'enviée', 'par', 'tant', 'de', 'nos', 'voisins']\n"
     ]
    }
   ],
   "source": [
    "print(new_tokenized[4])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "spiritual-beatles",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['au', 'aux', 'avec', 'ce', 'ces', 'dans', 'de', 'des', 'du', 'elle', 'en', 'et', 'eux', 'il', 'ils', 'je', 'la', 'le', 'les', 'leur']\n"
     ]
    }
   ],
   "source": [
    "#Stop words\n",
    "\n",
    "#On importe les mots \"inutiles\" (stopwords) du langage français\n",
    "from nltk.corpus import stopwords \n",
    "stop_words = stopwords.words(\"french\")\n",
    "print(stop_words[:20])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "revolutionary-story",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NOS MOTS 'INUTILES' : \n",
      " ['alinéa', 'amendement', 'º', 'L.', 'loi', 'présent', 'rapport', '1', '2020', 'II', 'phrase', '2', 'Gouvernement', 'I.', '575', 'organismes', '3', '2019', 'secteur', 'non', 'mot', 'mesure', 'Etat', 'Objet', 'objet', 'compte', 'situation', 'ans', 'propose', '4', 'III', 'également', 'congé', 'ainsi', 'afin', 'tel', 'cet']\n"
     ]
    }
   ],
   "source": [
    "#On importe nos propres stopwords\n",
    "SW = pd.read_csv('Added_stop_words.csv')\n",
    "SW = list(SW['StopWords'])\n",
    "print(\"NOS MOTS 'INUTILES' : \\n\", SW)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "hydraulic-reconstruction",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AVANT : \n",
      " ['Le', 'premier', 'alinéa', 'de', 'l', '’', 'article', 'L.', '521‑1', 'du', 'code', 'de', 'la', 'sécurité', 'sociale', 'est', 'complété', 'par', 'une', 'phrase', 'ainsi', 'rédigée', ':', '«', 'Elles', 'sont', 'universelles', '.', '»', 'Pendant', 'plus', 'de', 'cinquante', 'ans', ',', 'notre', 'politique', 'familiale', 'a', 'reposé', 'sur', 'le', 'principe', 'de', 'l', '’', 'universalité', '.', 'Cela', 'signifie', 'qu', '’', 'elle', 's', '’', 'adressait', 'à', 'tous', 'les', 'Français', ',', 'sans', 'distinction', 'sociale', '.', 'Elle', 'reposait', 'sur', 'l', '’', 'idée', 'que', 'chaque', 'enfant', 'à', 'naître', 'est', 'une', 'chance', 'et', 'une', 'richesse', 'pour', 'la', 'France', ',', 'pour', 'son', 'avenir', ',', 'quel', 'que', 'soient', 'les', 'ressources', 'dont', 'disposent', 'les', 'parents.Pour', 'mettre', 'en', 'place', 'ce', 'principe', 'd', '’', 'universalité', ',', 'la', 'politique', 'familiale', 'appelle', 'des', 'outils', 'dits', 'de', 'redistribution', 'horizontale', ',', 'c', '’', 'est-à-dire', 'des', 'mécanismes', 'de', 'solidarité', 'des', 'familles', 'sans', 'enfant', 'envers', 'les', 'familles', 'avec', 'enfants', ',', 'pour', 'que', ',', 'quel', 'que', 'soit', 'les', 'revenus', 'des', 'parents', ',', 'la', 'naissance', 'd', '’', 'un', 'enfant', 'n', '’', 'ait', 'pas', 'pour', 'effet', 'de', 'porter', 'atteinte', 'à', 'leur', 'niveau', 'de', 'vie.Or', ',', 'ce', 'principe', 'd', '’', 'universalité', 'a', 'été', 'mis', 'à', 'mal', 'sous', 'le', 'précédent', 'quinquennat', ',', 'notamment', 'à', 'travers', 'la', 'modulation', 'des', 'allocations', 'familiales', 'et', 'les', 'baisses', 'successives', 'du', 'quotient', 'familial', ',', 'faisant', 'ainsi', 'de', 'la', 'politique', 'familiale', 'une', 'politique', 'sociale', 'comme', 'les', 'autres.La', 'loi', 'de', 'financement', 'de', 'la', 'sécurité', 'sociale', 'pour', '2018', 'a', 'choisi', 'de', 'ne', 'pas', 'revenir', 'sur', 'cette', 'modulation', 'des', 'allocations', 'familiales', 'qui', 'a', 'pour', 'conséquence', 'principale', 'la', 'baisse', 'de', 'la', 'natalité', 'dans', 'notre', 'pays.Aussi', ',', 'le', 'présent', 'amendement', 'vise', 'à', 'réintroduire', 'ce', 'principe', 'd', '’', 'universalité', ',', 'base', 'même', 'de', 'la', 'politique', 'familiale', 'française', ',', 'enviée', 'par', 'tant', 'de', 'nos', 'voisins', '.'] \n",
      "\n",
      "APRÈS : \n",
      " ['premier', 'article', 'code', 'sécurité', 'sociale', 'complété', 'rédigée', 'elles', 'universelles', 'pendant', 'plus', 'cinquante', 'politique', 'familiale', 'a', 'reposé', 'principe', 'universalité', 'cela', 'signifie', 'adressait', 'tous', 'français', 'sans', 'distinction', 'sociale', 'reposait', 'idée', 'chaque', 'enfant', 'naître', 'chance', 'richesse', 'france', 'avenir', 'quel', 'ressources', 'dont', 'disposent', 'mettre', 'place', 'principe', 'universalité', 'politique', 'familiale', 'appelle', 'outils', 'dits', 'redistribution', 'horizontale', 'mécanismes', 'solidarité', 'familles', 'sans', 'enfant', 'envers', 'familles', 'enfants', 'quel', 'revenus', 'parents', 'naissance', 'enfant', 'effet', 'porter', 'atteinte', 'niveau', 'principe', 'universalité', 'a', 'mis', 'mal', 'sous', 'précédent', 'quinquennat', 'notamment', 'travers', 'modulation', 'allocations', 'familiales', 'baisses', 'successives', 'quotient', 'familial', 'faisant', 'politique', 'familiale', 'politique', 'sociale', 'comme', 'financement', 'sécurité', 'sociale', 'a', 'choisi', 'revenir', 'cette', 'modulation', 'allocations', 'familiales', 'a', 'conséquence', 'principale', 'baisse', 'natalité', 'vise', 'réintroduire', 'principe', 'universalité', 'base', 'politique', 'familiale', 'française', 'enviée', 'tant', 'voisins'] \n",
      "\n",
      "MOT ENLEVÉS : \n",
      " ['alinéa', 'amendement', 'º', 'L.', 'loi', 'présent', 'rapport', '1', '2020', 'II', 'phrase', '2', 'Gouvernement', 'I.', '575', 'organismes', '3', '2019', 'secteur', 'non', 'mot', 'mesure', 'Etat', 'Objet', 'objet', 'compte', 'situation', 'ans', 'propose', '4', 'III', 'également', 'congé', 'ainsi', 'afin', 'tel', 'cet', 'au', 'aux', 'avec', 'ce', 'ces', 'dans', 'de', 'des', 'du', 'elle', 'en', 'et', 'eux', 'il', 'ils', 'je', 'la', 'le', 'les', 'leur', 'lui', 'ma', 'mais', 'me', 'même', 'mes', 'moi', 'mon', 'ne', 'nos', 'notre', 'nous', 'on', 'ou', 'par', 'pas', 'pour', 'qu', 'que', 'qui', 'sa', 'se', 'ses', 'son', 'sur', 'ta', 'te', 'tes', 'toi', 'ton', 'tu', 'un', 'une', 'vos', 'votre', 'vous', 'c', 'd', 'j', 'l', 'à', 'm', 'n', 's', 't', 'y', 'été', 'étée', 'étées', 'étés', 'étant', 'étante', 'étants', 'étantes', 'suis', 'es', 'est', 'sommes', 'êtes', 'sont', 'serai', 'seras', 'sera', 'serons', 'serez', 'seront', 'serais', 'serait', 'serions', 'seriez', 'seraient', 'étais', 'était', 'étions', 'étiez', 'étaient', 'fus', 'fut', 'fûmes', 'fûtes', 'furent', 'sois', 'soit', 'soyons', 'soyez', 'soient', 'fusse', 'fusses', 'fût', 'fussions', 'fussiez', 'fussent', 'ayant', 'ayante', 'ayantes', 'ayants', 'eu', 'eue', 'eues', 'eus', 'ai', 'as', 'avons', 'avez', 'ont', 'aurai', 'auras', 'aura', 'aurons', 'aurez', 'auront', 'aurais', 'aurait', 'aurions', 'auriez', 'auraient', 'avais', 'avait', 'avions', 'aviez', 'avaient', 'eut', 'eûmes', 'eûtes', 'eurent', 'aie', 'aies', 'ait', 'ayons', 'ayez', 'aient', 'eusse', 'eusses', 'eût', 'eussions', 'eussiez', 'eussent']\n"
     ]
    }
   ],
   "source": [
    "#Stop Word removal\n",
    "print(\"AVANT : \\n\", tokenized[4], \"\\n\")\n",
    "final_SW = SW + stop_words\n",
    "tokenized = []\n",
    "for token in new_tokenized:\n",
    "    tokenized.append([ word for word in token if word not in final_SW])\n",
    "print(\"APRÈS : \\n\", tokenized[4], \"\\n\")\n",
    "\n",
    "print(\"MOT ENLEVÉS : \\n\", final_SW) # On affiche ci-dessous tous les mots que l'on a retirés"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "disabled-angola",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to /home/ben/nltk_data...\n",
      "[nltk_data]   Unzipping corpora/wordnet.zip.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('wordnet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "rising-alabama",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['premier', 'article', 'code', 'sécurité', 'sociale', 'complété', 'rédigée', 'elles', 'universelles', 'pendant', 'plus', 'cinquante', 'politique', 'familiale', 'a', 'reposé', 'principe', 'universalité', 'cela', 'signifie', 'adressait', 'tous', 'français', 'sans', 'distinction', 'sociale', 'reposait', 'idée', 'chaque', 'enfant', 'naître', 'chance', 'richesse', 'france', 'avenir', 'quel', 'ressources', 'dont', 'disposent', 'mettre', 'place', 'principe', 'universalité', 'politique', 'familiale', 'appelle', 'outils', 'dit', 'redistribution', 'horizontale', 'mécanismes', 'solidarité', 'familles', 'sans', 'enfant', 'envers', 'familles', 'enfants', 'quel', 'revenus', 'parent', 'naissance', 'enfant', 'effet', 'porter', 'atteinte', 'niveau', 'principe', 'universalité', 'a', 'mi', 'mal', 'sou', 'précédent', 'quinquennat', 'notamment', 'travers', 'modulation', 'allocation', 'familiales', 'bai', 'successives', 'quotient', 'familial', 'faisant', 'politique', 'familiale', 'politique', 'sociale', 'comme', 'financement', 'sécurité', 'sociale', 'a', 'choisi', 'revenir', 'cette', 'modulation', 'allocation', 'familiales', 'a', 'conséquence', 'principale', 'baisse', 'natalité', 'vise', 'réintroduire', 'principe', 'universalité', 'base', 'politique', 'familiale', 'française', 'enviée', 'tant', 'voisins']\n"
     ]
    }
   ],
   "source": [
    "#Lemmatization: on reduit les mots à leur racine\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "amdt_clean= []\n",
    "for token in tokenized:\n",
    "    amdt_clean.append([lemmatizer.lemmatize(word) for word in token])\n",
    "print(amdt_clean[4])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "royal-pioneer",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame(amdt_clean)\n",
    "#print(df.head(10))\n",
    "df.to_csv('amdt_clean.csv')  #Chaque ligne est un amendement"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "moving-custom",
   "metadata": {},
   "source": [
    "# Les mots les plus cités"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ancient-canon",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                     0                 1                2   \\\n",
      "0         (salariés, 7)  (entreprises, 4)       (moins, 4)   \n",
      "1       (naissance, 15)      (enfant, 14)       (prime, 9)   \n",
      "2  (expérimentation, 5)      (article, 4)        (code, 3)   \n",
      "3          (service, 7)      (article, 5)        (code, 4)   \n",
      "4        (politique, 5)      (sociale, 4)   (familiale, 4)   \n",
      "5       (allocation, 6)   (familiales, 5)  (modulation, 5)   \n",
      "6       (allocation, 4)     (adoptées, 3)   (naissance, 3)   \n",
      "7       (allocation, 4)     (adoptées, 3)   (naissance, 3)   \n",
      "8           (charge, 9)      (patient, 8)        (pied, 6)   \n",
      "9     (substitution, 5)   (pharmacien, 4)  (médicament, 4)   \n",
      "\n",
      "                   3                    4                  5               6   \\\n",
      "0          (prime, 3)  (exceptionnelle, 3)       (article, 3)       (mise, 3)   \n",
      "1        (sociale, 6)       (politique, 6)       (article, 5)       (date, 5)   \n",
      "2        (service, 3)           (cette, 2)     (notamment, 2)     (effets, 2)   \n",
      "3  (établissement, 4)           (actes, 4)         (soins, 3)     (social, 3)   \n",
      "4              (a, 4)        (principe, 4)  (universalité, 4)     (enfant, 3)   \n",
      "5         (revenu, 4)    (universalité, 4)       (sociale, 4)          (a, 4)   \n",
      "6           (mode, 3)         (mesures, 2)     (politique, 2)   (quotient, 2)   \n",
      "7           (mode, 3)         (mesures, 2)     (politique, 2)   (quotient, 2)   \n",
      "8          (prise, 5)      (diabétique, 5)       (diabète, 4)  (assurance, 3)   \n",
      "9          (cette, 4)         (article, 3)    (biologique, 3)      (cadre, 3)   \n",
      "\n",
      "                     7                    8                    9   ...  \\\n",
      "0            (place, 3)          (accord, 3)       (employeur, 2)  ...   \n",
      "1        (versement, 5)           (avant, 5)        (familles, 5)  ...   \n",
      "2  (contemporanéité, 2)   (participation, 2)      (financière, 2)  ...   \n",
      "3           (règles, 3)  (établissements, 3)           (entre, 3)  ...   \n",
      "4         (sécurité, 2)            (sans, 2)            (quel, 2)  ...   \n",
      "5          (revenus, 4)            (plus, 4)           (impôt, 4)  ...   \n",
      "6         (parental, 2)            (base, 2)          (nombre, 2)  ...   \n",
      "7         (parental, 2)            (base, 2)          (nombre, 2)  ...   \n",
      "8          (maladie, 3)          (cahier, 3)  (établissements, 3)  ...   \n",
      "9        (condition, 2)          (décret, 2)   (réglementaire, 2)  ...   \n",
      "\n",
      "                  20                  21              22              23  \\\n",
      "0    (mentionnés, 2)              (a, 2)       (code, 2)    (général, 2)   \n",
      "1       (arrivée, 3)           (bien, 3)      (après, 2)     (versée, 2)   \n",
      "2          (saad, 2)           (prix, 2)        (apa, 2)        (pch, 2)   \n",
      "3     (modalités, 2)  (qualification, 2)       (aide, 2)       (lors, 2)   \n",
      "4  (universelles, 1)        (pendant, 1)       (plus, 1)  (cinquante, 1)   \n",
      "5           (sou, 2)         (retour, 2)     (prévue, 2)  (familiale, 2)   \n",
      "6         (entre, 1)          (porte, 1)  (notamment, 1)   (familial, 1)   \n",
      "7         (entre, 1)          (porte, 1)  (notamment, 1)   (familial, 1)   \n",
      "8         (alors, 2)       (pratique, 2)       (plus, 2)       (déjà, 2)   \n",
      "9       (sociale, 1)        (prévoit, 1)       (code, 1)      (santé, 1)   \n",
      "\n",
      "                   24                25               26              27  \\\n",
      "0  (intéressement, 2)            (i, 1)       (après, 1)    (insérer, 1)   \n",
      "1     (ressources, 2)         (doit, 2)      (décret, 2)   (décembre, 2)   \n",
      "2       (convient, 2)           (ni, 2)   (compléter, 1)   (suivante, 1)   \n",
      "3   (expérimenter, 2)  (dérogations, 2)      (formes, 2)   (parcours, 2)   \n",
      "4         (reposé, 1)         (cela, 1)    (signifie, 1)  (adressait, 1)   \n",
      "5        (système, 2)        (effet, 2)     (montant, 2)      (foyer, 2)   \n",
      "6     (familiales, 1)        (prime, 1)  (prestation, 1)    (accueil, 1)   \n",
      "7     (familiales, 1)        (prime, 1)  (prestation, 1)    (accueil, 1)   \n",
      "8   (représentent, 2)      (élèvent, 2)        (donc, 2)      (soins, 2)   \n",
      "9       (publique, 1)      (indique, 1)  (dérogation, 1)    (premier, 1)   \n",
      "\n",
      "                    28              29  \n",
      "0         (suivant, 1)          (v, 1)  \n",
      "1            (mois, 2)      (choix, 2)  \n",
      "2    (gouvernement, 1)      (remet, 1)  \n",
      "3       (personnes, 2)      (sujet, 2)  \n",
      "4            (tous, 1)   (français, 1)  \n",
      "5  (redistribution, 2)    (partage, 2)  \n",
      "6           (jeune, 1)     (enfant, 1)  \n",
      "7           (jeune, 1)     (enfant, 1)  \n",
      "8         (million, 2)  (améliorer, 1)  \n",
      "9            (peut, 1)   (délivrer, 1)  \n",
      "\n",
      "[10 rows x 30 columns]\n"
     ]
    }
   ],
   "source": [
    "#Counting\n",
    "from collections import Counter\n",
    "\n",
    "most = []\n",
    "for token in amdt_clean:\n",
    "    bow = Counter(token)\n",
    "    most.append(bow.most_common(30))\n",
    "\n",
    "\n",
    "df = pd.DataFrame(most)\n",
    "print(df.head(10))\n",
    "df.to_csv('most_common_word_per_amdt.csv')  #Chaque ligne est un amendement \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "second-theory",
   "metadata": {},
   "source": [
    "# TF IDF\n",
    "Term frequency - inverse document frequency"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "binary-publicity",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['compléter suivante gouvernement remet parlement échéance cette période expérimentation évaluation portant notamment effets contemporanéité crédit impôt mentionné article sexdecies code général impôts participation financière bénéficiaires prestations mentionnées article code action sociale familles part coûts induits application i article prestataires définis article code travail participant expérimentation autre part louable expérimenter dispositifs permettant plus avoir décalage entre moment où dépense réalisée celui où aide perçues personnes recourant service domicile notamment plus vulnérables entre elles nécessaire mesurer éventuels effets induits négatifs telles mesures effet certains saad facturent prix participation financière légale prévue apa pch convient mesurer effet cette expérimentation prix service conséquence reste ailleurs expérimentation prévoit contemporanéité apa pch organisé centre cesu or ore déjà conseils départementaux peuvent verser directement mécanisme tier payant prises charge saad limitant avance trésorerie bénéficiaires convient vérifier expérimentation introduisant nouvel acteur engendre ni surcoût personnes service prestataires ni complexité supplémentaire', 'après insérer cinq alinéas suivants article dudit code limitent possibilités organiser délégations soins cadre prise charge établissement santé établissement service social rédigé disposition suivantes code action sociale familles a règles tarification organisation prévues code action sociale familles applicables établissements service mentionnés article code b article redéfinir modalités qualification entre actes aide actes soins lors prise charge établissement service social permet amplifier portée article effet question clé rigidités financières lesquelles article ouvre possibilité expérimenter dérogations règles tarification établissements service sociaux autres formes cloisonnement mettent cause pertinence accompagnements génèrent rupture parcours personnes prévoit étendre dérogations deux point règles organisation établissements service sociaux favoriser innovation organisationnelles permettre expérimenter nouvelles formes coopération entre acteurs sociaux sanitaires service parcours santé vie personnes modalités qualification entre actes aide actes soins cadre accompagnement établissement service social plf similaire déposé rejeté gouvernement motif sujet devait être traité lors concertation grand âge autonomie pilotée dominique libault or mar aborde expressément sujet justifie déposer nouveau']\n"
     ]
    }
   ],
   "source": [
    "#Mise sous forme 'corpus'\n",
    "corpus = []\n",
    "for amdt1 in amdt_clean:\n",
    "    temp = ' '.join(amdt1)\n",
    "    corpus.append(temp)\n",
    "    temp = ''\n",
    "    \n",
    "#Vectorization - Term Frequency in Global Corpus\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "vectorizer = CountVectorizer()\n",
    "tf = vectorizer.fit_transform(corpus)\n",
    "\n",
    "print(corpus[2:4])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bound-james",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['aa', 'aah', 'aasctel', 'ab', 'abaissant', 'abaisse', 'abaissement', 'abaisser', 'abaissés', 'abandon']\n"
     ]
    }
   ],
   "source": [
    "#Features\n",
    "feature_names = vectorizer.get_feature_names()\n",
    "print(feature_names[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "perfect-korean",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['AMANR5L15PO420120B2296P0D1N000001', 'AMANR5L15PO420120B2296P0D1N000002', 'AMANR5L15PO420120B2296P0D1N000005', 'AMANR5L15PO420120B2296P0D1N000008', 'AMANR5L15PO420120B2296P0D1N000010']\n"
     ]
    }
   ],
   "source": [
    "#Génération du corpus index\n",
    "import re\n",
    "corpus_index = amdt['uid'].tolist()\n",
    "print(corpus_index[:5])\n",
    "#amdt['uid'].nunique() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "honest-depression",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      0         1      2      3      4      5      6      7      8      9      \\\n",
      "0       0.0  0.000000    0.0    0.0    0.0    0.0    0.0    0.0    0.0    0.0   \n",
      "1       0.0  0.000000    0.0    0.0    0.0    0.0    0.0    0.0    0.0    0.0   \n",
      "2       0.0  0.000000    0.0    0.0    0.0    0.0    0.0    0.0    0.0    0.0   \n",
      "3       0.0  0.000000    0.0    0.0    0.0    0.0    0.0    0.0    0.0    0.0   \n",
      "4       0.0  0.000000    0.0    0.0    0.0    0.0    0.0    0.0    0.0    0.0   \n",
      "...     ...       ...    ...    ...    ...    ...    ...    ...    ...    ...   \n",
      "4792    0.0  0.000000    0.0    0.0    0.0    0.0    0.0    0.0    0.0    0.0   \n",
      "4793    0.0  0.000000    0.0    0.0    0.0    0.0    0.0    0.0    0.0    0.0   \n",
      "4794    0.0  5.669292    0.0    0.0    0.0    0.0    0.0    0.0    0.0    0.0   \n",
      "4795    0.0  0.000000    0.0    0.0    0.0    0.0    0.0    0.0    0.0    0.0   \n",
      "4796    0.0  0.000000    0.0    0.0    0.0    0.0    0.0    0.0    0.0    0.0   \n",
      "\n",
      "      ...  13052    13053     13054  13055  13056  13057  13058  13059  13060  \\\n",
      "0     ...    0.0  2.09383  0.000000    0.0    0.0    0.0    0.0    0.0    0.0   \n",
      "1     ...    0.0  2.09383  0.000000    0.0    0.0    0.0    0.0    0.0    0.0   \n",
      "2     ...    0.0  0.00000  0.000000    0.0    0.0    0.0    0.0    0.0    0.0   \n",
      "3     ...    0.0  2.09383  0.000000    0.0    0.0    0.0    0.0    0.0    0.0   \n",
      "4     ...    0.0  0.00000  0.000000    0.0    0.0    0.0    0.0    0.0    0.0   \n",
      "...   ...    ...      ...       ...    ...    ...    ...    ...    ...    ...   \n",
      "4792  ...    0.0  0.00000  0.000000    0.0    0.0    0.0    0.0    0.0    0.0   \n",
      "4793  ...    0.0  2.09383  8.377342    0.0    0.0    0.0    0.0    0.0    0.0   \n",
      "4794  ...    0.0  0.00000  0.000000    0.0    0.0    0.0    0.0    0.0    0.0   \n",
      "4795  ...    0.0  0.00000  0.000000    0.0    0.0    0.0    0.0    0.0    0.0   \n",
      "4796  ...    0.0  0.00000  0.000000    0.0    0.0    0.0    0.0    0.0    0.0   \n",
      "\n",
      "      13061  \n",
      "0       0.0  \n",
      "1       0.0  \n",
      "2       0.0  \n",
      "3       0.0  \n",
      "4       0.0  \n",
      "...     ...  \n",
      "4792    0.0  \n",
      "4793    0.0  \n",
      "4794    0.0  \n",
      "4795    0.0  \n",
      "4796    0.0  \n",
      "\n",
      "[4797 rows x 13062 columns]\n",
      "   0      1      2      3      4      5      6      7      8      9      ...  \\\n",
      "0    0.0    0.0    0.0    0.0    0.0    0.0    0.0    0.0    0.0    0.0  ...   \n",
      "\n",
      "   13052    13053  13054  13055  13056  13057  13058  13059  13060  13061  \n",
      "0    0.0  2.09383    0.0    0.0    0.0    0.0    0.0    0.0    0.0    0.0  \n",
      "\n",
      "[1 rows x 13062 columns]\n",
      "                                     aa  aah  aasctel  ab  abaissant  abaisse  \\\n",
      "AMANR5L15PO420120B2296P0D1N000001   NaN  NaN      NaN NaN        NaN      NaN   \n",
      "AMANR5L15PO420120B2296P0D1N000002   NaN  NaN      NaN NaN        NaN      NaN   \n",
      "AMANR5L15PO420120B2296P0D1N000005   NaN  NaN      NaN NaN        NaN      NaN   \n",
      "AMANR5L15PO420120B2296P0D1N000008   NaN  NaN      NaN NaN        NaN      NaN   \n",
      "AMANR5L15PO420120B2296P0D1N000010   NaN  NaN      NaN NaN        NaN      NaN   \n",
      "...                                  ..  ...      ...  ..        ...      ...   \n",
      "AMANR5L15PO717460BTA0353P0D1N000118 NaN  NaN      NaN NaN        NaN      NaN   \n",
      "AMANR5L15PO717460BTA0353P0D1N000119 NaN  NaN      NaN NaN        NaN      NaN   \n",
      "AMANR5L15PO717460BTA0353P0D1N000120 NaN  NaN      NaN NaN        NaN      NaN   \n",
      "AMANR5L15PO717460BTA0353P0D1N000121 NaN  NaN      NaN NaN        NaN      NaN   \n",
      "AMANR5L15PO717460BTA0353P0D1N000122 NaN  NaN      NaN NaN        NaN      NaN   \n",
      "\n",
      "                                     abaissement  abaisser  abaissés  abandon  \\\n",
      "AMANR5L15PO420120B2296P0D1N000001            NaN       NaN       NaN      NaN   \n",
      "AMANR5L15PO420120B2296P0D1N000002            NaN       NaN       NaN      NaN   \n",
      "AMANR5L15PO420120B2296P0D1N000005            NaN       NaN       NaN      NaN   \n",
      "AMANR5L15PO420120B2296P0D1N000008            NaN       NaN       NaN      NaN   \n",
      "AMANR5L15PO420120B2296P0D1N000010            NaN       NaN       NaN      NaN   \n",
      "...                                          ...       ...       ...      ...   \n",
      "AMANR5L15PO717460BTA0353P0D1N000118          NaN       NaN       NaN      NaN   \n",
      "AMANR5L15PO717460BTA0353P0D1N000119          NaN       NaN       NaN      NaN   \n",
      "AMANR5L15PO717460BTA0353P0D1N000120          NaN       NaN       NaN      NaN   \n",
      "AMANR5L15PO717460BTA0353P0D1N000121          NaN       NaN       NaN      NaN   \n",
      "AMANR5L15PO717460BTA0353P0D1N000122          NaN       NaN       NaN      NaN   \n",
      "\n",
      "                                     ...  événements  être  êtrepar  île  \\\n",
      "AMANR5L15PO420120B2296P0D1N000001    ...         NaN   NaN      NaN  NaN   \n",
      "AMANR5L15PO420120B2296P0D1N000002    ...         NaN   NaN      NaN  NaN   \n",
      "AMANR5L15PO420120B2296P0D1N000005    ...         NaN   NaN      NaN  NaN   \n",
      "AMANR5L15PO420120B2296P0D1N000008    ...         NaN   NaN      NaN  NaN   \n",
      "AMANR5L15PO420120B2296P0D1N000010    ...         NaN   NaN      NaN  NaN   \n",
      "...                                  ...         ...   ...      ...  ...   \n",
      "AMANR5L15PO717460BTA0353P0D1N000118  ...         NaN   NaN      NaN  NaN   \n",
      "AMANR5L15PO717460BTA0353P0D1N000119  ...         NaN   NaN      NaN  NaN   \n",
      "AMANR5L15PO717460BTA0353P0D1N000120  ...         NaN   NaN      NaN  NaN   \n",
      "AMANR5L15PO717460BTA0353P0D1N000121  ...         NaN   NaN      NaN  NaN   \n",
      "AMANR5L15PO717460BTA0353P0D1N000122  ...         NaN   NaN      NaN  NaN   \n",
      "\n",
      "                                     œnologie  œufs  œuvrant  œuvre  œuvrent  \\\n",
      "AMANR5L15PO420120B2296P0D1N000001         NaN   NaN      NaN    NaN      NaN   \n",
      "AMANR5L15PO420120B2296P0D1N000002         NaN   NaN      NaN    NaN      NaN   \n",
      "AMANR5L15PO420120B2296P0D1N000005         NaN   NaN      NaN    NaN      NaN   \n",
      "AMANR5L15PO420120B2296P0D1N000008         NaN   NaN      NaN    NaN      NaN   \n",
      "AMANR5L15PO420120B2296P0D1N000010         NaN   NaN      NaN    NaN      NaN   \n",
      "...                                       ...   ...      ...    ...      ...   \n",
      "AMANR5L15PO717460BTA0353P0D1N000118       NaN   NaN      NaN    NaN      NaN   \n",
      "AMANR5L15PO717460BTA0353P0D1N000119       NaN   NaN      NaN    NaN      NaN   \n",
      "AMANR5L15PO717460BTA0353P0D1N000120       NaN   NaN      NaN    NaN      NaN   \n",
      "AMANR5L15PO717460BTA0353P0D1N000121       NaN   NaN      NaN    NaN      NaN   \n",
      "AMANR5L15PO717460BTA0353P0D1N000122       NaN   NaN      NaN    NaN      NaN   \n",
      "\n",
      "                                     œuvres  \n",
      "AMANR5L15PO420120B2296P0D1N000001       NaN  \n",
      "AMANR5L15PO420120B2296P0D1N000002       NaN  \n",
      "AMANR5L15PO420120B2296P0D1N000005       NaN  \n",
      "AMANR5L15PO420120B2296P0D1N000008       NaN  \n",
      "AMANR5L15PO420120B2296P0D1N000010       NaN  \n",
      "...                                     ...  \n",
      "AMANR5L15PO717460BTA0353P0D1N000118     NaN  \n",
      "AMANR5L15PO717460BTA0353P0D1N000119     NaN  \n",
      "AMANR5L15PO717460BTA0353P0D1N000120     NaN  \n",
      "AMANR5L15PO717460BTA0353P0D1N000121     NaN  \n",
      "AMANR5L15PO717460BTA0353P0D1N000122     NaN  \n",
      "\n",
      "[4797 rows x 13062 columns]\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>aa</th>\n",
       "      <th>aah</th>\n",
       "      <th>aasctel</th>\n",
       "      <th>ab</th>\n",
       "      <th>abaissant</th>\n",
       "      <th>abaisse</th>\n",
       "      <th>abaissement</th>\n",
       "      <th>abaisser</th>\n",
       "      <th>abaissés</th>\n",
       "      <th>abandon</th>\n",
       "      <th>...</th>\n",
       "      <th>événements</th>\n",
       "      <th>être</th>\n",
       "      <th>êtrepar</th>\n",
       "      <th>île</th>\n",
       "      <th>œnologie</th>\n",
       "      <th>œufs</th>\n",
       "      <th>œuvrant</th>\n",
       "      <th>œuvre</th>\n",
       "      <th>œuvrent</th>\n",
       "      <th>œuvres</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>AMANR5L15PO420120B2296P0D1N000001</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>AMANR5L15PO420120B2296P0D1N000002</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>AMANR5L15PO420120B2296P0D1N000005</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>AMANR5L15PO420120B2296P0D1N000008</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>AMANR5L15PO420120B2296P0D1N000010</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>AMANR5L15PO717460BTA0353P0D1N000118</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>AMANR5L15PO717460BTA0353P0D1N000119</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>AMANR5L15PO717460BTA0353P0D1N000120</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>AMANR5L15PO717460BTA0353P0D1N000121</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>AMANR5L15PO717460BTA0353P0D1N000122</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>4797 rows × 13062 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                      aa  aah  aasctel   ab  abaissant  \\\n",
       "AMANR5L15PO420120B2296P0D1N000001    0.0  0.0      0.0  0.0        0.0   \n",
       "AMANR5L15PO420120B2296P0D1N000002    0.0  0.0      0.0  0.0        0.0   \n",
       "AMANR5L15PO420120B2296P0D1N000005    0.0  0.0      0.0  0.0        0.0   \n",
       "AMANR5L15PO420120B2296P0D1N000008    0.0  0.0      0.0  0.0        0.0   \n",
       "AMANR5L15PO420120B2296P0D1N000010    0.0  0.0      0.0  0.0        0.0   \n",
       "...                                  ...  ...      ...  ...        ...   \n",
       "AMANR5L15PO717460BTA0353P0D1N000118  0.0  0.0      0.0  0.0        0.0   \n",
       "AMANR5L15PO717460BTA0353P0D1N000119  0.0  0.0      0.0  0.0        0.0   \n",
       "AMANR5L15PO717460BTA0353P0D1N000120  0.0  0.0      0.0  0.0        0.0   \n",
       "AMANR5L15PO717460BTA0353P0D1N000121  0.0  0.0      0.0  0.0        0.0   \n",
       "AMANR5L15PO717460BTA0353P0D1N000122  0.0  0.0      0.0  0.0        0.0   \n",
       "\n",
       "                                     abaisse  abaissement  abaisser  abaissés  \\\n",
       "AMANR5L15PO420120B2296P0D1N000001        0.0          0.0       0.0       0.0   \n",
       "AMANR5L15PO420120B2296P0D1N000002        0.0          0.0       0.0       0.0   \n",
       "AMANR5L15PO420120B2296P0D1N000005        0.0          0.0       0.0       0.0   \n",
       "AMANR5L15PO420120B2296P0D1N000008        0.0          0.0       0.0       0.0   \n",
       "AMANR5L15PO420120B2296P0D1N000010        0.0          0.0       0.0       0.0   \n",
       "...                                      ...          ...       ...       ...   \n",
       "AMANR5L15PO717460BTA0353P0D1N000118      0.0          0.0       0.0       0.0   \n",
       "AMANR5L15PO717460BTA0353P0D1N000119      0.0          0.0       0.0       0.0   \n",
       "AMANR5L15PO717460BTA0353P0D1N000120      0.0          0.0       0.0       0.0   \n",
       "AMANR5L15PO717460BTA0353P0D1N000121      0.0          0.0       0.0       0.0   \n",
       "AMANR5L15PO717460BTA0353P0D1N000122      0.0          0.0       0.0       0.0   \n",
       "\n",
       "                                     abandon  ...  événements  être  êtrepar  \\\n",
       "AMANR5L15PO420120B2296P0D1N000001        0.0  ...         0.0   0.0      0.0   \n",
       "AMANR5L15PO420120B2296P0D1N000002        0.0  ...         0.0   0.0      0.0   \n",
       "AMANR5L15PO420120B2296P0D1N000005        0.0  ...         0.0   0.0      0.0   \n",
       "AMANR5L15PO420120B2296P0D1N000008        0.0  ...         0.0   0.0      0.0   \n",
       "AMANR5L15PO420120B2296P0D1N000010        0.0  ...         0.0   0.0      0.0   \n",
       "...                                      ...  ...         ...   ...      ...   \n",
       "AMANR5L15PO717460BTA0353P0D1N000118      0.0  ...         0.0   0.0      0.0   \n",
       "AMANR5L15PO717460BTA0353P0D1N000119      0.0  ...         0.0   0.0      0.0   \n",
       "AMANR5L15PO717460BTA0353P0D1N000120      0.0  ...         0.0   0.0      0.0   \n",
       "AMANR5L15PO717460BTA0353P0D1N000121      0.0  ...         0.0   0.0      0.0   \n",
       "AMANR5L15PO717460BTA0353P0D1N000122      0.0  ...         0.0   0.0      0.0   \n",
       "\n",
       "                                     île  œnologie  œufs  œuvrant  œuvre  \\\n",
       "AMANR5L15PO420120B2296P0D1N000001    0.0       0.0   0.0      0.0    0.0   \n",
       "AMANR5L15PO420120B2296P0D1N000002    0.0       0.0   0.0      0.0    0.0   \n",
       "AMANR5L15PO420120B2296P0D1N000005    0.0       0.0   0.0      0.0    0.0   \n",
       "AMANR5L15PO420120B2296P0D1N000008    0.0       0.0   0.0      0.0    0.0   \n",
       "AMANR5L15PO420120B2296P0D1N000010    0.0       0.0   0.0      0.0    0.0   \n",
       "...                                  ...       ...   ...      ...    ...   \n",
       "AMANR5L15PO717460BTA0353P0D1N000118  0.0       0.0   0.0      0.0    0.0   \n",
       "AMANR5L15PO717460BTA0353P0D1N000119  0.0       0.0   0.0      0.0    0.0   \n",
       "AMANR5L15PO717460BTA0353P0D1N000120  0.0       0.0   0.0      0.0    0.0   \n",
       "AMANR5L15PO717460BTA0353P0D1N000121  0.0       0.0   0.0      0.0    0.0   \n",
       "AMANR5L15PO717460BTA0353P0D1N000122  0.0       0.0   0.0      0.0    0.0   \n",
       "\n",
       "                                     œuvrent  œuvres  \n",
       "AMANR5L15PO420120B2296P0D1N000001        0.0     0.0  \n",
       "AMANR5L15PO420120B2296P0D1N000002        0.0     0.0  \n",
       "AMANR5L15PO420120B2296P0D1N000005        0.0     0.0  \n",
       "AMANR5L15PO420120B2296P0D1N000008        0.0     0.0  \n",
       "AMANR5L15PO420120B2296P0D1N000010        0.0     0.0  \n",
       "...                                      ...     ...  \n",
       "AMANR5L15PO717460BTA0353P0D1N000118      0.0     0.0  \n",
       "AMANR5L15PO717460BTA0353P0D1N000119      0.0     0.0  \n",
       "AMANR5L15PO717460BTA0353P0D1N000120      0.0     0.0  \n",
       "AMANR5L15PO717460BTA0353P0D1N000121      0.0     0.0  \n",
       "AMANR5L15PO717460BTA0353P0D1N000122      0.0     0.0  \n",
       "\n",
       "[4797 rows x 13062 columns]"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## LAST STOP HERE: POURQUOI A-T-ON UNIQUEMENT DES NAN ?\n",
    "\n",
    "#IDF - Inverse Document Frequencies\n",
    "import numpy as np\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "# initialize and fit TfidfVectorizer\n",
    "vectorizer = TfidfVectorizer(norm=None)\n",
    "tf_idf_scores = vectorizer.fit_transform(corpus)\n",
    "#print(tf_idf_scores)\n",
    "# get vocabulary of terms\n",
    "feature_names = vectorizer.get_feature_names()\n",
    "#print(type(tf_idf_scores.todense()))\n",
    "print (pd.DataFrame(tf_idf_scores.todense()) )  #ICI on a bien les bonnes valeurs\n",
    "print(pd.DataFrame(tf_idf_scores.todense()[0][0]))\n",
    "#print(feature_names)\n",
    "# create pandas DataFrame with tf-idf scores\n",
    "df_tf_idf = pd.DataFrame(data = pd.DataFrame(tf_idf_scores.todense()), index=corpus_index, columns=feature_names)\n",
    "print(df_tf_idf)\n",
    "#df_tf_idf.fillna(0)\n",
    "df_tf_idf.replace(np.nan, 0)\n",
    "#df_tf_idf.reset_index() \n",
    "#df_tf_idf.rename_axis('ID') \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "documented-rebecca",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "nan\n",
      "13062\n",
      "<class 'pandas.core.series.Series'>\n",
      "nan\n",
      "<class 'pandas.core.series.Series'>\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "  Most_frequent_words  Word_occurrence\n",
      "0                  aa              NaN\n",
      "1                 aah              NaN\n",
      "2             aasctel              NaN\n",
      "3                  ab              NaN\n",
      "4           abaissant              NaN\n",
      "  Most_frequent_words  Word_occurrence\n",
      "0                  aa              NaN\n",
      "1                 aah              NaN\n",
      "2             aasctel              NaN\n",
      "3                  ab              NaN\n",
      "4           abaissant              NaN\n",
      "nan\n",
      "13062\n",
      "<class 'pandas.core.series.Series'>\n",
      "nan\n",
      "<class 'pandas.core.series.Series'>\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "  Most_frequent_words  Word_occurrence\n",
      "0                  aa              NaN\n",
      "1                 aah              NaN\n",
      "2             aasctel              NaN\n",
      "3                  ab              NaN\n",
      "4           abaissant              NaN\n",
      "  Most_frequent_words  Word_occurrence\n",
      "0                  aa              NaN\n",
      "1                 aah              NaN\n",
      "2             aasctel              NaN\n",
      "3                  ab              NaN\n",
      "4           abaissant              NaN\n",
      "nan\n",
      "13062\n",
      "<class 'pandas.core.series.Series'>\n",
      "nan\n",
      "<class 'pandas.core.series.Series'>\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "  Most_frequent_words  Word_occurrence\n",
      "0                  aa              NaN\n",
      "1                 aah              NaN\n",
      "2             aasctel              NaN\n",
      "3                  ab              NaN\n",
      "4           abaissant              NaN\n",
      "  Most_frequent_words  Word_occurrence\n",
      "0                  aa              NaN\n",
      "1                 aah              NaN\n",
      "2             aasctel              NaN\n",
      "3                  ab              NaN\n",
      "4           abaissant              NaN\n",
      "nan\n",
      "13062\n",
      "<class 'pandas.core.series.Series'>\n",
      "nan\n",
      "<class 'pandas.core.series.Series'>\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "  Most_frequent_words  Word_occurrence\n",
      "0                  aa              NaN\n",
      "1                 aah              NaN\n",
      "2             aasctel              NaN\n",
      "3                  ab              NaN\n",
      "4           abaissant              NaN\n",
      "  Most_frequent_words  Word_occurrence\n",
      "0                  aa              NaN\n",
      "1                 aah              NaN\n",
      "2             aasctel              NaN\n",
      "3                  ab              NaN\n",
      "4           abaissant              NaN\n",
      "nan\n",
      "13062\n",
      "<class 'pandas.core.series.Series'>\n",
      "nan\n",
      "<class 'pandas.core.series.Series'>\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "  Most_frequent_words  Word_occurrence\n",
      "0                  aa              NaN\n",
      "1                 aah              NaN\n",
      "2             aasctel              NaN\n",
      "3                  ab              NaN\n",
      "4           abaissant              NaN\n",
      "  Most_frequent_words  Word_occurrence\n",
      "0                  aa              NaN\n",
      "1                 aah              NaN\n",
      "2             aasctel              NaN\n",
      "3                  ab              NaN\n",
      "4           abaissant              NaN\n"
     ]
    }
   ],
   "source": [
    "for n in range(5):#range(len(df_tf_idf)):\n",
    "    #print(n)\n",
    "    #print(id)\n",
    "    main = pd.DataFrame()\n",
    "    main['Most_frequent_words'] = feature_names\n",
    "    main['Word_occurrence'] = df_tf_idf.iloc[n]  # MAIN ISSUE HERE: Series definition not taking float but convert2 nan\n",
    "    print(max(df_tf_idf.iloc[n]))\n",
    "    print(len(df_tf_idf.iloc[n]))\n",
    "    print(type(df_tf_idf.iloc[n]))\n",
    "    print(max(main['Word_occurrence']))\n",
    "    print(type(main['Word_occurrence']))\n",
    "    print(type(main))\n",
    "    print(main.head(5))\n",
    "    main.sort_values(by =['Word_occurrence'], inplace = True, ascending = False, na_position='last')\n",
    "    print(main.head(5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "amended-generator",
   "metadata": {},
   "outputs": [],
   "source": [
    "#exclude\n",
    "#Organizing most common words !Il faut transposer\n",
    "# tuple_mots = ()\n",
    "# for n in range(len(df_tf_idf)):\n",
    "#     print(n)\n",
    "#     id = df_tf_idf.index[n]\n",
    "#     print(id)\n",
    "#     #Creating a DF per amendment\n",
    "#     main = pd.DataFrame()\n",
    "#     main['Mots principaux'] = df_tf_idf.columns.T\n",
    "#     main['Occurence'] = df_tf_idf.iloc[n]\n",
    "    \n",
    "    \n",
    "#     #main['mots'] = df_tf_idf['ID']\n",
    "    \n",
    "#     print(max(df_tf_idf.index[n]))\n",
    "#     #print(main.head(5))\n",
    "#     #main.sort_values(main[id])\n",
    "\n",
    "#     #main = main.iloc[:25] #On ne garde que les 25 mots les plus utilisés par amendement\n",
    "#     #tuple_mots.append(main)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "furnished-chrome",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(4797, 13062)"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_tf_idf.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "celtic-joshua",
   "metadata": {},
   "outputs": [],
   "source": [
    "id = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dental-riding",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'AMANR5L15PO420120B2296P0D1N000001'"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_tf_idf.index[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "powerful-caution",
   "metadata": {},
   "outputs": [],
   "source": [
    "test = df_tf_idf.index[id]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "played-savage",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "leximpact",
   "language": "python",
   "name": "leximpact"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
