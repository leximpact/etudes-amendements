{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "medieval-danger",
   "metadata": {},
   "source": [
    "## Etude des sujets des amendements PLFSS - Sasha\n",
    "\n",
    "## Préparation des données"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "encouraging-simpson",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!python -m pip install --upgrade pip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "direct-receipt",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install -q --upgrade pandas  spacy  more_itertools"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "brief-telescope",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Put these at the top of every notebook, to get automatic reloading and inline plotting\n",
    "%reload_ext autoreload\n",
    "%autoreload 2\n",
    "from IPython.core.interactiveshell import InteractiveShell\n",
    "InteractiveShell.ast_node_interactivity = \"all\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "occasional-logan",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33mWARNING: You are using pip version 20.1.1; however, version 21.0.1 is available.\r\n",
      "You should consider upgrading via the '/Library/Frameworks/Python.framework/Versions/3.7/bin/python -m pip install --upgrade pip' command.\u001b[0m\r\n"
     ]
    }
   ],
   "source": [
    "!python -m spacy download fr_core_news_sm > /dev/null"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "structural-windsor",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>texteLegislatifUid</th>\n",
       "      <th>uid</th>\n",
       "      <th>avantAApres</th>\n",
       "      <th>dispositif</th>\n",
       "      <th>exposeSommaire</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>PRJLANR5L15B2296</td>\n",
       "      <td>AMANR5L15PO420120B2296P0D1N000001</td>\n",
       "      <td>A</td>\n",
       "      <td>I. – Après l’alinéa 13, insérer l’alinéa suiv...</td>\n",
       "      <td>La mise en place d’un accord d’intéressement d...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>PRJLANR5L15B2296</td>\n",
       "      <td>AMANR5L15PO420120B2296P0D1N000002</td>\n",
       "      <td>Apres</td>\n",
       "      <td>À la première phrase du premier alinéa de l’ar...</td>\n",
       "      <td>L’article L 531‑2 du Code de la Sécurité Socia...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>PRJLANR5L15B2296</td>\n",
       "      <td>AMANR5L15PO420120B2296P0D1N000005</td>\n",
       "      <td>A</td>\n",
       "      <td>Compléter l’alinéa 17 par la phrase suivante :...</td>\n",
       "      <td>S’il est louable d’expérimenter des dispositif...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>PRJLANR5L15B2296</td>\n",
       "      <td>AMANR5L15PO420120B2296P0D1N000008</td>\n",
       "      <td>A</td>\n",
       "      <td>Après l’alinéa 8, insérer les cinq alinéas sui...</td>\n",
       "      <td>Cet amendement permet d’amplifier la portée de...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>PRJLANR5L15B2296</td>\n",
       "      <td>AMANR5L15PO420120B2296P0D1N000010</td>\n",
       "      <td>Apres</td>\n",
       "      <td>Le premier alinéa de l’article L. 521‑1 du cod...</td>\n",
       "      <td>Pendant plus de cinquante ans, notre politique...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  texteLegislatifUid                                uid avantAApres  \\\n",
       "0   PRJLANR5L15B2296  AMANR5L15PO420120B2296P0D1N000001           A   \n",
       "1   PRJLANR5L15B2296  AMANR5L15PO420120B2296P0D1N000002       Apres   \n",
       "2   PRJLANR5L15B2296  AMANR5L15PO420120B2296P0D1N000005           A   \n",
       "3   PRJLANR5L15B2296  AMANR5L15PO420120B2296P0D1N000008           A   \n",
       "4   PRJLANR5L15B2296  AMANR5L15PO420120B2296P0D1N000010       Apres   \n",
       "\n",
       "                                          dispositif  \\\n",
       "0   I. – Après l’alinéa 13, insérer l’alinéa suiv...   \n",
       "1  À la première phrase du premier alinéa de l’ar...   \n",
       "2  Compléter l’alinéa 17 par la phrase suivante :...   \n",
       "3  Après l’alinéa 8, insérer les cinq alinéas sui...   \n",
       "4  Le premier alinéa de l’article L. 521‑1 du cod...   \n",
       "\n",
       "                                      exposeSommaire  \n",
       "0  La mise en place d’un accord d’intéressement d...  \n",
       "1  L’article L 531‑2 du Code de la Sécurité Socia...  \n",
       "2  S’il est louable d’expérimenter des dispositif...  \n",
       "3  Cet amendement permet d’amplifier la portée de...  \n",
       "4  Pendant plus de cinquante ans, notre politique...  "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Importing modules\n",
    "import pandas as pd\n",
    "#amdt = pd.read_csv('https://github.com/leximpact/donnees-extraites-assemblee/blob/main/textes_amendements_nouveaux_articles_plfss_2020-2021.csv')\n",
    "amdt = pd.read_csv('https://raw.githubusercontent.com/leximpact/donnees-extraites-assemblee/main/textes_amendements_nouveaux_articles_plfss_2020-2021.csv')\n",
    "amdt.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "continued-doctrine",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4797"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(amdt) #Nombre total d'amendements"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "incredible-search",
   "metadata": {},
   "outputs": [],
   "source": [
    "#On regroupe dans un même texte chaque dispositif et son exposé sommaire\n",
    "amdt['texte'] = amdt['dispositif'] + amdt['exposeSommaire'] "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "improving-boutique",
   "metadata": {},
   "source": [
    "## Nettoyage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "controlling-treasure",
   "metadata": {},
   "outputs": [],
   "source": [
    "#pip install nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "returning-progressive",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to /Users/sasha/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /Users/sasha/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('stopwords')\n",
    "nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "unusual-cathedral",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "developmental-japan",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Tokenisation := découpage du texte en listes de mots\n",
    "from nltk.tokenize import word_tokenize\n",
    "tokenized = [ word_tokenize(text) for text in amdt['texte'] ] #return_token"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "surprising-effort",
   "metadata": {},
   "source": [
    "# Normalization"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "gorgeous-marshall",
   "metadata": {},
   "source": [
    "### Retrait des mots de liaison (_stopword_) et de la ponctuation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "average-vector",
   "metadata": {},
   "outputs": [],
   "source": [
    "# #Removing punctuation AND Casing\n",
    "#( Casing: Est-ce vraiment utile ? Est-ce qu'on ne va pas perdre les Acronymes de vue ?)\n",
    "new_tokenized = []\n",
    "for token in tokenized:\n",
    "    new_tokenized.append([ word.lower() for word in token if word.isalpha()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "hairy-train",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['le', 'premier', 'alinéa', 'de', 'l', 'article', 'du', 'code', 'de', 'la', 'sécurité', 'sociale', 'est', 'complété', 'par', 'une', 'phrase', 'ainsi', 'rédigée', 'elles', 'sont', 'universelles', 'pendant', 'plus', 'de', 'cinquante', 'ans', 'notre', 'politique', 'familiale', 'a', 'reposé', 'sur', 'le', 'principe', 'de', 'l', 'universalité', 'cela', 'signifie', 'qu', 'elle', 's', 'adressait', 'à', 'tous', 'les', 'français', 'sans', 'distinction', 'sociale', 'elle', 'reposait', 'sur', 'l', 'idée', 'que', 'chaque', 'enfant', 'à', 'naître', 'est', 'une', 'chance', 'et', 'une', 'richesse', 'pour', 'la', 'france', 'pour', 'son', 'avenir', 'quel', 'que', 'soient', 'les', 'ressources', 'dont', 'disposent', 'les', 'mettre', 'en', 'place', 'ce', 'principe', 'd', 'universalité', 'la', 'politique', 'familiale', 'appelle', 'des', 'outils', 'dits', 'de', 'redistribution', 'horizontale', 'c', 'des', 'mécanismes', 'de', 'solidarité', 'des', 'familles', 'sans', 'enfant', 'envers', 'les', 'familles', 'avec', 'enfants', 'pour', 'que', 'quel', 'que', 'soit', 'les', 'revenus', 'des', 'parents', 'la', 'naissance', 'd', 'un', 'enfant', 'n', 'ait', 'pas', 'pour', 'effet', 'de', 'porter', 'atteinte', 'à', 'leur', 'niveau', 'de', 'ce', 'principe', 'd', 'universalité', 'a', 'été', 'mis', 'à', 'mal', 'sous', 'le', 'précédent', 'quinquennat', 'notamment', 'à', 'travers', 'la', 'modulation', 'des', 'allocations', 'familiales', 'et', 'les', 'baisses', 'successives', 'du', 'quotient', 'familial', 'faisant', 'ainsi', 'de', 'la', 'politique', 'familiale', 'une', 'politique', 'sociale', 'comme', 'les', 'loi', 'de', 'financement', 'de', 'la', 'sécurité', 'sociale', 'pour', 'a', 'choisi', 'de', 'ne', 'pas', 'revenir', 'sur', 'cette', 'modulation', 'des', 'allocations', 'familiales', 'qui', 'a', 'pour', 'conséquence', 'principale', 'la', 'baisse', 'de', 'la', 'natalité', 'dans', 'notre', 'le', 'présent', 'amendement', 'vise', 'à', 'réintroduire', 'ce', 'principe', 'd', 'universalité', 'base', 'même', 'de', 'la', 'politique', 'familiale', 'française', 'enviée', 'par', 'tant', 'de', 'nos', 'voisins']\n"
     ]
    }
   ],
   "source": [
    "print(new_tokenized[4])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "altered-chorus",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NOS MOTS 'INUTILES' : \n",
      " \n",
      " ['au', 'aux', 'avec', 'ce', 'ces', 'dans', 'de', 'des', 'du', 'elle', 'en', 'et', 'eux', 'il', 'ils', 'je', 'la', 'le', 'les', 'leur', 'lui', 'ma', 'mais', 'me', 'même', 'mes', 'moi', 'mon', 'ne', 'nos', 'notre', 'nous', 'on', 'ou', 'par', 'pas', 'pour', 'qu', 'que', 'qui', 'sa', 'se', 'ses', 'son', 'sur', 'ta', 'te', 'tes', 'toi', 'ton', 'tu', 'un', 'une', 'vos', 'votre', 'vous', 'c', 'd', 'j', 'l', 'à', 'm', 'n', 's', 't', 'y', 'été', 'étée', 'étées', 'étés', 'étant', 'étante', 'étants', 'étantes', 'suis', 'es', 'est', 'sommes', 'êtes', 'sont', 'serai', 'seras', 'sera', 'serons', 'serez', 'seront', 'serais', 'serait', 'serions', 'seriez', 'seraient', 'étais', 'était', 'étions', 'étiez', 'étaient', 'fus', 'fut', 'fûmes', 'fûtes', 'furent', 'sois', 'soit', 'soyons', 'soyez', 'soient', 'fusse', 'fusses', 'fût', 'fussions', 'fussiez', 'fussent', 'ayant', 'ayante', 'ayantes', 'ayants', 'eu', 'eue', 'eues', 'eus', 'ai', 'as', 'avons', 'avez', 'ont', 'aurai', 'auras', 'aura', 'aurons', 'aurez', 'auront', 'aurais', 'aurait', 'aurions', 'auriez', 'auraient', 'avais', 'avait', 'avions', 'aviez', 'avaient', 'eut', 'eûmes', 'eûtes', 'eurent', 'aie', 'aies', 'ait', 'ayons', 'ayez', 'aient', 'eusse', 'eusses', 'eût', 'eussions', 'eussiez', 'eussent', 'vôtre', 'abord', 'eux-mêmes', 'car', 'ah', 'huit', \"j'\", 'désormais', 'sixième', 'puis', 'cela', 'anterieur', 'bat', 'merci', 'moindres', 'faisaient', 'lors', 'relativement', 'parce', 'moi-même', 'quant-à-soi', 'celles', 'egale', \"t'\", 'derriere', 'dix-sept', 'egalement', 'alors', 'plutôt', 'hue', 'laisser', 'doit', 'aupres', 'neuvième', 'troisièmement', 'divers', 'dehors', 'celui-ci', 'voici', 'celle', 'cent', 'depuis', 'peu', 'ci', 'lorsque', 'vôtres', 'hors', 'parmi', 'quatre', 'possible', 'hou', 'quelconque', 'nombreux', 'toute', \"s'\", 'mien', 'elles-mêmes', 'pendant', 'specifique', \"l'\", 'semblaient', 'douzième', 'c’', 'semble', 'cinquième', 'fais', 'nouvea', 'celle-ci', 'vous-mêmes', 'tenant', 'outre', 'directe', 'lesquelles', 'etaient', 'pouvait', 'hé', 'diverse', 'laquelle', 'autre', 'dite', 'lesquels', 'ça', 'ceux', 'sent', 'miens', 'puisque', 'certes', 'jusque', 'seulement', 'tellement', 'douze', 'près', 'quelques', 'cependant', \"m'\", 'attendu', 'soixante', 'autrement', 'vé', 'peux', 'a', 'certaines', 'possibles', 'celles-là', 'mille', 'seul', 'revoilà', 'touchant', 'derrière', 'semblable', 'souvent', 'après', 'exactement', 'autrui', 'seize', 'compris', 'parseme', 'vais', 'parler', 'quarante', 'ha', 'desquels', 'eh', 'etait', 'donc', 'pres', 'debout', \"qu'\", 'basee', 'sinon', 'mienne', 'pourrait', \"quelqu'un\", 'parfois', 'ouias', 'maint', 'chez', 'telle', 'directement', 'ouvert', 'suffisant', 'cinquante', 'dix-huit', 'da', 'relative', 'avant', 'première', 'être', 'proche', 'fait', 'suivre', 'dits', 'différents', 'tout', 'onze', 'etais', 'lès', 'anterieures', 'comme', 'chaque', 'diverses', 'dix', 'plusieurs', 'soi-même', 'procedant', 'suivante', 'desquelles', 'oust', 'quinze', 'quand', 'tant', 'hep', 'sait', 'unes', 'façon', 'entre', 'auquel', 'neanmoins', 'suffit', 'duquel', 'ainsi', 'doivent', 'durant', 'quatre-vingt', 'aussi', 'allons', 'ni', 'vont', 'certain', 'pourrais', \"aujourd'hui\", 'etant', 'chacune', 'meme', 'm’', 'afin', 'chacun', 'nôtre', 'quoi', 'vers', 'anterieure', 'peuvent', 'quanta', 'different', 'seule', 'auxquelles', 'suivantes', 'ès', 'quatorze', 'font', 'siennes', 'troisième', 'excepté', 'ceux-là', 'environ', 'j’', 'ô', 'té', 'voilà', 'vu', 'tres', 'néanmoins', 'elle-même', 'pu', 'faisant', 'selon', 'treize', 'certaine', 'différentes', 'longtemps', 'où', 's’', 'siens', 'revoici', 'personne', 'rend', 'septième', 'deuxième', \"n'\", 'lequel', 'peut', 'là', 'devra', 'retour', 'certains', 'etc', 'moi-meme', 'tous', 'quant', \"c'\", 'combien', 'avoir', 'uns', 'restent', 'ceux-ci', 'malgre', 'onzième', 'dixième', 'sept', 'leurs', 'nôtres', 'quel', 'comment', 'tien', 'cinquantaine', 'desormais', 'si', 'i', 'l’', 'tiennes', 'celles-ci', 'deja', 'miennes', 'dessous', 'apres', 'ceci', 'dire', 'tend', 'precisement', 'soi', 'dès', 'celui', 'tienne', 'quoique', 'toujours', 'hormis', 'delà', 'egales', 'sans', 'quiconque', 'qu’', 'cinq', 'différente', 'premièrement', 'effet', 'hui', 'quelque', 'assez', 'quatrième', 'mêmes', 'toutes', 'parlent', 'maintenant', 'lui-même', 'telles', 'dedans', 'quelles', 'houp', 'tiens', 'juste', 'differents', 'dix-neuf', 'surtout', 'aucun', 'ouverts', 'trois', 'celle-là', 'ho', 'moins', 'nombreuses', 'différent', 'lui-meme', 'partant', 'nous-mêmes', 'pense', 'plus', 'memes', 'sauf', 'devant', 'auxquels', 'pourquoi', 'quatrièmement', 'va', 'encore', 'six', 'importe', 'permet', 'notamment', 'devers', 'stop', 'toi-même', 'dessus', 'specifiques', 'tenir', 'cinquantième', \"d'\", 'cet', 'deuxièmement', 'premier', 'â', 'allaient', 'bas', 'hi', 'prealable', 'n’', 'sous', 'ouverte', 'nul', 'd’', 'suivants', 'aucune', 'tente', 'elles', 'huitième', 'enfin', 'aujourd', 'feront', 'autres', 'vas', 'etre', 'hem', 'suffisante', 'differentes', 'dit', 'reste', 'via', 't’', 'semblent', 'tel', 'parle', 'celui-là', 'cette', 'tels', 'deux', 'rendre', 'dont', 'vingt', 'concernant', 'quels', 'trente', 'jusqu', 'gens', 'suit', 'o', 'envers', 'restant', 'na', 'suivant', 'quelle', 'sienne', 'sien', 'ore', 'alinéa', 'amendement', 'º', 'L.', 'loi', 'présent', 'rapport', '1', '2020', 'II', 'phrase', '2', 'Gouvernement', 'I.', '575', 'organismes', '3', '2019', 'secteur', 'non', 'mot', 'mesure', 'Etat', 'Objet', 'objet', 'compte', 'situation', 'ans', 'propose', '4', 'III', 'également', 'congé'] \n",
      "\n",
      "AVANT : \n",
      " \n",
      " ['Le', 'premier', 'alinéa', 'de', 'l', '’', 'article', 'L.', '521‑1', 'du', 'code', 'de', 'la', 'sécurité', 'sociale', 'est', 'complété', 'par', 'une', 'phrase', 'ainsi', 'rédigée', ':', '«', 'Elles', 'sont', 'universelles', '.', '»', 'Pendant', 'plus', 'de', 'cinquante', 'ans', ',', 'notre', 'politique', 'familiale', 'a', 'reposé', 'sur', 'le', 'principe', 'de', 'l', '’', 'universalité', '.', 'Cela', 'signifie', 'qu', '’', 'elle', 's', '’', 'adressait', 'à', 'tous', 'les', 'Français', ',', 'sans', 'distinction', 'sociale', '.', 'Elle', 'reposait', 'sur', 'l', '’', 'idée', 'que', 'chaque', 'enfant', 'à', 'naître', 'est', 'une', 'chance', 'et', 'une', 'richesse', 'pour', 'la', 'France', ',', 'pour', 'son', 'avenir', ',', 'quel', 'que', 'soient', 'les', 'ressources', 'dont', 'disposent', 'les', 'parents.Pour', 'mettre', 'en', 'place', 'ce', 'principe', 'd', '’', 'universalité', ',', 'la', 'politique', 'familiale', 'appelle', 'des', 'outils', 'dits', 'de', 'redistribution', 'horizontale', ',', 'c', '’', 'est-à-dire', 'des', 'mécanismes', 'de', 'solidarité', 'des', 'familles', 'sans', 'enfant', 'envers', 'les', 'familles', 'avec', 'enfants', ',', 'pour', 'que', ',', 'quel', 'que', 'soit', 'les', 'revenus', 'des', 'parents', ',', 'la', 'naissance', 'd', '’', 'un', 'enfant', 'n', '’', 'ait', 'pas', 'pour', 'effet', 'de', 'porter', 'atteinte', 'à', 'leur', 'niveau', 'de', 'vie.Or', ',', 'ce', 'principe', 'd', '’', 'universalité', 'a', 'été', 'mis', 'à', 'mal', 'sous', 'le', 'précédent', 'quinquennat', ',', 'notamment', 'à', 'travers', 'la', 'modulation', 'des', 'allocations', 'familiales', 'et', 'les', 'baisses', 'successives', 'du', 'quotient', 'familial', ',', 'faisant', 'ainsi', 'de', 'la', 'politique', 'familiale', 'une', 'politique', 'sociale', 'comme', 'les', 'autres.La', 'loi', 'de', 'financement', 'de', 'la', 'sécurité', 'sociale', 'pour', '2018', 'a', 'choisi', 'de', 'ne', 'pas', 'revenir', 'sur', 'cette', 'modulation', 'des', 'allocations', 'familiales', 'qui', 'a', 'pour', 'conséquence', 'principale', 'la', 'baisse', 'de', 'la', 'natalité', 'dans', 'notre', 'pays.Aussi', ',', 'le', 'présent', 'amendement', 'vise', 'à', 'réintroduire', 'ce', 'principe', 'd', '’', 'universalité', ',', 'base', 'même', 'de', 'la', 'politique', 'familiale', 'française', ',', 'enviée', 'par', 'tant', 'de', 'nos', 'voisins', '.'] \n",
      "\n",
      "APRÈS : \n",
      " \n",
      " ['article', 'code', 'sécurité', 'sociale', 'complété', 'rédigée', 'universelles', 'politique', 'familiale', 'reposé', 'principe', 'universalité', 'signifie', 'adressait', 'français', 'distinction', 'sociale', 'reposait', 'idée', 'enfant', 'naître', 'chance', 'richesse', 'france', 'avenir', 'ressources', 'disposent', 'mettre', 'place', 'principe', 'universalité', 'politique', 'familiale', 'appelle', 'outils', 'redistribution', 'horizontale', 'mécanismes', 'solidarité', 'familles', 'enfant', 'familles', 'enfants', 'revenus', 'parents', 'naissance', 'enfant', 'porter', 'atteinte', 'niveau', 'principe', 'universalité', 'mis', 'mal', 'précédent', 'quinquennat', 'travers', 'modulation', 'allocations', 'familiales', 'baisses', 'successives', 'quotient', 'familial', 'politique', 'familiale', 'politique', 'sociale', 'financement', 'sécurité', 'sociale', 'choisi', 'revenir', 'modulation', 'allocations', 'familiales', 'conséquence', 'principale', 'baisse', 'natalité', 'vise', 'réintroduire', 'principe', 'universalité', 'base', 'politique', 'familiale', 'française', 'enviée', 'voisins'] \n",
      "\n"
     ]
    }
   ],
   "source": [
    "#STOP WORDS\n",
    "import spacy\n",
    "#On importe les mots \"inutiles\" (stopwords) du langage français depuis Space -ET- NLTK\n",
    "from nltk.corpus import stopwords \n",
    "stop_words = stopwords.words(\"french\")\n",
    "\n",
    "from spacy.lang.fr.stop_words import STOP_WORDS as fr_stop\n",
    "for word in fr_stop:\n",
    "        stop_words.append(word)\n",
    "del(word)\n",
    "stop_words = list(stop_words)\n",
    "\n",
    "#On importe nos propres stopwords depuis le fichier CSV\n",
    "SW = pd.read_csv('Added_stop_words.csv')\n",
    "SW = list(SW.StopWords)\n",
    "final_SW = stop_words+ SW\n",
    "\n",
    "#On enlève les doublons\n",
    "df = pd.DataFrame({\"StopWords\" : final_SW})\n",
    "final_SW = df.drop_duplicates()\n",
    "final_SW = list(final_SW.StopWords)\n",
    "print(\"NOS MOTS 'INUTILES' : \\n \\n\", final_SW ,'\\n')\n",
    "\n",
    "#STOP WORD REMOVAL\n",
    "print(\"AVANT : \\n \\n\", tokenized[4], \"\\n\")\n",
    "tokenized = []\n",
    "for token in new_tokenized:\n",
    "    tokenized.append([ word for word in token if word not in final_SW])\n",
    "print(\"APRÈS : \\n \\n\", tokenized[4], \"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "neural-lawsuit",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to /Users/sasha/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('wordnet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "accompanied-dating",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Lemmatization: on reduit les mots à leur racine\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "#amdt_clean= []\n",
    "#for token in tokenized:\n",
    "#    amdt_clean.append([lemmatizer.lemmatize(word) for word in token])\n",
    "#print(amdt_clean[4])\n",
    "\n",
    "#La lemmatization de nltk est nulle en français. Il faudrait pouvoir réussir à importer celle de Spacy\n",
    "import spacy\n",
    "nlp = spacy.load(\"fr_core_news_md\")\n",
    "\n",
    "#amdt_clean= []\n",
    "#for token in tokenized:\n",
    "#    amdt_clean.append([nlp(word) for word in token])\n",
    "#print(\"Ci-dessous les mots réduits à leur racine: \\n \\n\", amdt_clean[4])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "helpful-license",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "exempt-christianity",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mangée\n",
      "mangées\n",
      "mangé\n",
      "mangés\n",
      "mangez\n",
      "mangeons\n",
      "joliment\n",
      "eaten\n",
      "voile\n",
      "wa\n"
     ]
    }
   ],
   "source": [
    "#Cellule de test\n",
    "print(lemmatizer.lemmatize('mangée'))\n",
    "print(lemmatizer.lemmatize('mangées'))\n",
    "print(lemmatizer.lemmatize('mangé'))\n",
    "print(lemmatizer.lemmatize('mangés'))\n",
    "print(lemmatizer.lemmatize('mangez'))\n",
    "print(lemmatizer.lemmatize('mangeons'))\n",
    "print(lemmatizer.lemmatize('joliment'))\n",
    "print(lemmatizer.lemmatize('eaten'))\n",
    "print(lemmatizer.lemmatize('voiles'))\n",
    "print(lemmatizer.lemmatize('was'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "ecological-animal",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'voudr'"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mang\n",
      "mang\n",
      "mang\n",
      "mang\n",
      "mang\n",
      "mangeon\n",
      "jol\n",
      "eaten\n",
      "voil\n",
      "was\n"
     ]
    }
   ],
   "source": [
    "from nltk.stem.snowball import FrenchStemmer\n",
    "stemmer = FrenchStemmer()\n",
    "stemmer.stem('voudrais')\n",
    "\n",
    "#Cellule de test\n",
    "print(stemmer.stem('mangée'))\n",
    "print(stemmer.stem('mangées'))\n",
    "print(stemmer.stem('mangé'))\n",
    "print(stemmer.stem('mangés'))\n",
    "print(stemmer.stem('mangez'))\n",
    "print(stemmer.stem('mangeons'))\n",
    "print(stemmer.stem('joliment'))\n",
    "print(stemmer.stem('eaten'))\n",
    "print(stemmer.stem('voiles'))\n",
    "print(stemmer.stem('was'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "applied-mixer",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'spacy_lefff'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-18-673af9139b4c>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m#from french_lefff_lemmatizer.french_lefff_lemmatizer import FrenchLefffLemmatizer\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mspacy\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mspacy_lefff\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mLefffLemmatizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mPOSTagger\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0mnlp\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mspacy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'fr'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mlemmatizer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mLefffLemmatizer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'spacy_lefff'"
     ]
    }
   ],
   "source": [
    "#from french_lefff_lemmatizer.french_lefff_lemmatizer import FrenchLefffLemmatizer\n",
    "import spacy\n",
    "from spacy_lefff import LefffLemmatizer, POSTagger\n",
    "nlp = spacy.load('fr')\n",
    "lemmatizer = LefffLemmatizer()\n",
    "print(lemmatizer.lemmatize('mangée'))\n",
    "print(lemmatizer.lemmatize('mangées'))\n",
    "print(lemmatizer.lemmatize('mangé'))\n",
    "print(lemmatizer.lemmatize('mangés'))\n",
    "print(lemmatizer.lemmatize('mangez'))\n",
    "print(lemmatizer.lemmatize('mangeons'))\n",
    "print(lemmatizer.lemmatize('joliment'))\n",
    "print(lemmatizer.lemmatize('eaten'))\n",
    "print(lemmatizer.lemmatize('voiles'))\n",
    "print(lemmatizer.lemmatize('was'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "automotive-representation",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Cellule de test\n",
    "nlp.add_pipe('lemmatizer')\n",
    "#print(nlp.lemmatizer(u'mangée')) #pk u ?\n",
    "print(nlp(u'mangées'))\n",
    "print(nlp('mangé'))\n",
    "print(nlp('mangés'))\n",
    "print(nlp('mangez'))\n",
    "print(nlp('mangeons'))\n",
    "print(nlp('joliment'))\n",
    "print(nlp('eaten'))\n",
    "print(nlp('voiles'))\n",
    "print(nlp('was'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "neither-county",
   "metadata": {},
   "outputs": [],
   "source": [
    "#On enregistre tous les textes en version \"clean\" pour pouvoir réutiliser cette data plus tard\n",
    "del(df)\n",
    "df = pd.DataFrame(data = amdt_clean )\n",
    "df = df.fillna('') \n",
    "print(df.head(10))\n",
    "df.to_csv('amdements_cleaned.csv')  #Chaque ligne est un amendement"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cleared-fishing",
   "metadata": {},
   "source": [
    "# Les mots les plus cités"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bronze-mason",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Counting\n",
    "from collections import Counter\n",
    "def most_common_words(amdt_clean, N):\n",
    "    freq = []\n",
    "    words = []\n",
    "    for token in amdt_clean:\n",
    "        bow = Counter(token)\n",
    "        freq.append(bow.most_common(N)) \n",
    "        #print(bow.most_common(N))\n",
    "        \n",
    "        words.append([bow.most_common(N)[i][0] for i in range(N-1)])\n",
    "    return pd.DataFrame(freq), words\n",
    "\n",
    "#On ne garde que les N mots les plus utilisés pour chaque amendement\n",
    "N = 10\n",
    "freq, words = most_common_words(amdt_clean, N)\n",
    "#On sauvegarde cette donnée dans un fichier .csv \n",
    "#dont les lignes sont les amdts et les colonnes sont les mots et leur fréquence\n",
    "freq.to_csv('most_common_{nb}_words_per_amdt.csv'.format(nb = N))\n",
    "\n",
    "print(\"Ci-dessous un exemple des mots les plus utilisés sur 5 amendements: \\n \\n \", freq.head(5))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "sound-alias",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analyse des mots du corpus dans sa globalité\n",
    "words = pd.DataFrame(words)\n",
    "mots_uniques = []\n",
    "for c in words:\n",
    "    temp = list (words[c].unique())\n",
    "    for word in temp:\n",
    "        mots_uniques.append( word )  #List of unique words\n",
    "        \n",
    "#On enlève les doublons\n",
    "mots_uniques = list(dict.fromkeys(mots_uniques))\n",
    "mots_uniques.sort()\n",
    "print( len(mots_uniques) )\n",
    "print(mots_uniques)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "communist-blanket",
   "metadata": {},
   "source": [
    "# TF IDF\n",
    "Term frequency - inverse document frequency"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "listed-disorder",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Mise sous forme 'corpus' (une liste de tous les textes)\n",
    "corpus = []\n",
    "for amdt1 in amdt_clean:\n",
    "    temp = ' '.join(amdt1)\n",
    "    corpus.append(temp)\n",
    "    temp = ''\n",
    "    \n",
    "#Vectorization - Term Frequency in Global Corpus\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "vectorizer = CountVectorizer()\n",
    "tf = vectorizer.fit_transform(corpus)\n",
    "\n",
    "#print(corpus[2:4])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "intended-authentication",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Features\n",
    "feature_names = vectorizer.get_feature_names()\n",
    "print(feature_names[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ordinary-loading",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Génération du corpus index\n",
    "import re\n",
    "corpus_index = amdt['uid'].tolist()\n",
    "print(corpus_index[:5])\n",
    "#amdt['uid'].nunique() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "prostate-economy",
   "metadata": {},
   "outputs": [],
   "source": [
    "## LAST STOP HERE: POURQUOI A-T-ON UNIQUEMENT DES NAN ?\n",
    "\n",
    "#IDF - Inverse Document Frequencies\n",
    "import numpy as np\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "# initialize and fit TfidfVectorizer\n",
    "vectorizer = TfidfVectorizer(norm=None)\n",
    "tf_idf_scores = vectorizer.fit_transform(corpus)\n",
    "#print(tf_idf_scores)\n",
    "# get vocabulary of terms\n",
    "feature_names = vectorizer.get_feature_names()\n",
    "#print(type(tf_idf_scores.todense()))\n",
    "print (pd.DataFrame(tf_idf_scores.todense()) )  #ICI on a bien les bonnes valeurs\n",
    "print(pd.DataFrame(tf_idf_scores.todense()[0][0]))\n",
    "#print(feature_names)\n",
    "# create pandas DataFrame with tf-idf scores\n",
    "df_tf_idf = pd.DataFrame(data = pd.DataFrame(tf_idf_scores.todense()), index=corpus_index, columns=feature_names)\n",
    "print(df_tf_idf)\n",
    "#df_tf_idf.fillna(0)\n",
    "df_tf_idf.replace(np.nan, 0)\n",
    "#df_tf_idf.reset_index() \n",
    "#df_tf_idf.rename_axis('ID') \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "lesbian-orchestra",
   "metadata": {},
   "outputs": [],
   "source": [
    "for n in range(5):#range(len(df_tf_idf)):\n",
    "    #print(n)\n",
    "    #print(id)\n",
    "    main = pd.DataFrame()\n",
    "    main['Most_frequent_words'] = feature_names\n",
    "    main['Word_occurrence'] = df_tf_idf.iloc[n]  # MAIN ISSUE HERE: Series definition not taking float but convert2 nan\n",
    "    print(max(df_tf_idf.iloc[n]))\n",
    "    print(len(df_tf_idf.iloc[n]))\n",
    "    print(type(df_tf_idf.iloc[n]))\n",
    "    print(max(main['Word_occurrence']))\n",
    "    print(type(main['Word_occurrence']))\n",
    "    print(type(main))\n",
    "    print(main.head(5))\n",
    "    main.sort_values(by =['Word_occurrence'], inplace = True, ascending = False, na_position='last')\n",
    "    print(main.head(5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "hispanic-running",
   "metadata": {},
   "outputs": [],
   "source": [
    "#exclude\n",
    "#Organizing most common words !Il faut transposer\n",
    "# tuple_mots = ()\n",
    "# for n in range(len(df_tf_idf)):\n",
    "#     print(n)\n",
    "#     id = df_tf_idf.index[n]\n",
    "#     print(id)\n",
    "#     #Creating a DF per amendment\n",
    "#     main = pd.DataFrame()\n",
    "#     main['Mots principaux'] = df_tf_idf.columns.T\n",
    "#     main['Occurence'] = df_tf_idf.iloc[n]\n",
    "    \n",
    "    \n",
    "#     #main['mots'] = df_tf_idf['ID']\n",
    "    \n",
    "#     print(max(df_tf_idf.index[n]))\n",
    "#     #print(main.head(5))\n",
    "#     #main.sort_values(main[id])\n",
    "\n",
    "#     #main = main.iloc[:25] #On ne garde que les 25 mots les plus utilisés par amendement\n",
    "#     #tuple_mots.append(main)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "infinite-kitchen",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_tf_idf.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fundamental-terrace",
   "metadata": {},
   "outputs": [],
   "source": [
    "id = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "light-rubber",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_tf_idf.index[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dedicated-georgia",
   "metadata": {},
   "outputs": [],
   "source": [
    "test = df_tf_idf.index[id]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "educational-valley",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "openfisca",
   "language": "python",
   "name": "openfisca"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
